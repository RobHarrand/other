{
    "contents" : "#https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/\n#http://amunategui.github.io/speak-like-a-doctor\n#https://cran.r-project.org/web/packages/quanteda/quanteda.pdf\n\noptions( java.parameters = \"-Xmx4g\" )\n\nlibrary(\"tm\")\nlibrary(\"SnowballC\")\nlibrary(\"stringr\")\nlibrary(\"readr\")\nlibrary('stringdist')\n\n#getwd()\n\n#Questions to consider\n\n#Some words are more frequent than others - what are the distributions of word frequencies?\n\n#What are the frequencies of 2-grams and 3-grams in the dataset?\n\n#How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?\n\n#How do you evaluate how many of the words come from foreign languages?\n\n#Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller \n#number of words in the dictionary to cover the same number of phrases?\n\n\n#Create Corpus\n#-----------------------------\n\n#docs_en = Corpus(DirSource(\"texts/en_US/\"))\n#docs_de = Corpus(DirSource(\"/home/rob/Desktop/Coursera/Capstone/texts/de_DE/\"))\n#docs_ru = Corpus(DirSource(\"/home/rob/Desktop/Coursera/Capstone/texts/ru_RU/\"))\n#docs_fl = Corpus(DirSource(\"/home/rob/Desktop/Coursera/Capstone/texts/fl_FL/\"))\n\n#Encoding(docs_en[[1]]$content)  <- \"UTF-8\"\n\nen_blogs = read_lines(\"texts/en_US/en_US.blogs.txt\")\nen_tweets = readLines(\"texts/en_US/en_US.twitter.txt\", skipNul = T)\nen_news = read_lines(\"texts/en_US/en_US.news.txt\")\ndict = read_lines(\"texts/Dict.txt\")\n#idioms = read_lines(\"idioms.txt\")\n\n# en_blogs_10pc[1:3]\n# en_tweets_10pc[1:3]\n# en_news_10pc[1:10]\n\nprop = 0.1\n\nset.seed(100)\nen_blogs_sample = sample(en_blogs, (length(en_blogs)*prop))\nset.seed(100)\nen_tweets_sample = sample(en_tweets, (length(en_tweets)*prop))\nset.seed(100)\nen_news_sample = sample(en_news, (length(en_news)*prop))\n\nen_blogs_sample = iconv(en_blogs_sample, \"latin1\", \"ASCII\", \"\")\nen_tweets_sample = iconv(en_tweets_sample, \"latin1\", \"ASCII\", \"\")\nen_news_sample = iconv(en_news_sample, \"latin1\", \"ASCII\", \"\")\n\n\n#Create test set\n\n# set.seed(500)\n# en_blogs_sample_test = sample(en_blogs, (length(en_blogs)*prop))\n# set.seed(500)\n# en_tweets_sample_test = sample(en_tweets, (length(en_tweets)*prop))\n# set.seed(500)\n# en_news_sample_test = sample(en_news, (length(en_news)*prop))\n# \n# en_blogs_sample_test = iconv(en_blogs_sample_test, \"latin1\", \"ASCII\", \"\")\n# en_tweets_sample_test = iconv(en_tweets_sample_test, \"latin1\", \"ASCII\", \"\")\n# en_news_sample_test = iconv(en_news_sample_test, \"latin1\", \"ASCII\", \"\")\n\n\n\n# dir.create(\"10pc_sample\", showWarnings = FALSE)\n# \n# write(en_blogs_10pc, \"10pc_sample/sample.blogs.txt\")\n# write(en_tweets_10pc, \"10_pcsample/sample.news.txt\")\n# write(en_news_10pc, \"10_pcsample/sample.twitter.txt\")\n\nrm(en_blogs)\nrm(en_tweets)\nrm(en_news)\n\nall_sample = c(en_blogs_sample, en_tweets_sample, en_news_sample)\nall_sample_corpus = Corpus(VectorSource(list(all_sample)))\n\n#all_sample_test = c(en_blogs_sample_test, en_tweets_sample_test, en_news_sample_test)\n#all_sample_corpus_test = Corpus(VectorSource(list(all_sample_test)))\n\nrm(en_blogs_sample)\nrm(en_tweets_sample)\nrm(en_news_sample)\n#rm(all_sample)\n\n#rm(en_blogs_sample_test)\n#rm(en_tweets_sample_test)\n#rm(en_news_sample_test)\n#rm(all_sample_test)\n\n\n#head(all_sample_corpus[[1]]$content)\n#head(all_sample_corpus_test[[1]]$content)\n\n\n\n#all_sample_corpus[1:3]\n\n#writeLines(as.character(all_sample_corpus[[1]]))\n\n# en_blogs_10pc_corpus = Corpus(VectorSource(en_blogs_10pc))\n# en_tweets_10pc_corpus = Corpus(VectorSource(en_tweets_10pc))\n# en_news_10pc_corpus = Corpus(VectorSource(en_news_10pc))\n# \n# tm_combine\n\n# head(en_blogs, 3)\n# \n# # Delete all the titles. They're not going to be useful 'next words' and the full-stops confuse the sentence endings,\n# en_blogs <- gsub(pattern='Mr.|Mrs.|Miss.|Ms.', x=en_blogs, replacement=' ')\n# \n# # swap all sentence ends with code 'zyxcba'\n# en_blogs <- gsub(pattern=';|\\\\. |!|\\\\?', x=en_blogs, replacement='zyxcba')\n#   \n# # remove all non-alpha text (numbers etc)\n# en_blogs <- gsub(pattern=\"[^[:alpha:]]\", x=en_blogs, replacement = ' ')\n#   \n# # force all characters to lower case\n# en_blogs <- tolower(en_blogs)\n#   \n# # remove any small words {size} or {min,max}\n# #en_blogs <- gsub(pattern=\"\\\\W*\\\\b\\\\w{1,2}\\\\b\", x=en_blogs, replacement=' ')\n#   \n# # remove contiguous spaces\n# en_blogs <- gsub(pattern=\"\\\\s+\", x=en_blogs, replacement=' ')\n#   \n# # split sentences by split code\n# en_blogs = unlist(strsplit(x=en_blogs, split='zyxcba',fixed = TRUE))\n# \n# head(en_blogs, 3)\n\n\n#Delete all the titles. They're not going to be useful 'next words' and the full-stops confuse the sentence endings.\n#To do this, create the TitletoSpace content transformer\n# TitletoSpace = content_transformer(function(x, pattern) {return (gsub(pattern, \" \", x))})\n# \n# #Apply this new function,\n# docs_en = tm_map(docs_en, TitletoSpace, \"Mr.\")\n# docs_en = tm_map(docs_en, TitletoSpace, \"Mrs.\")\n# docs_en = tm_map(docs_en, TitletoSpace, \"Ms.\")\n# docs_en = tm_map(docs_en, TitletoSpace, \"Miss.\")\n\n\n#Remove swearwords\n\nswearwords = read.delim(\"swearWords.txt\", sep = \"\\n\", header = F)\nall_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)\n#all_sample_corpus_test = tm_map(all_sample_corpus_test, removeWords, swearwords$V1)\nrm(swearwords)\n\n\n#Tidy\n#---------------------------------------\n\n#convertSpace = content_transformer(function(x, pattern) {return (gsub(pattern, \" \", x))})\n\n#There are still some uncommon punctuation marks remaining. Remove these with the custom function,\n#docs_en <- tm_map(docs_en, convertSpace, \"â€™\")\n#docs_en <- tm_map(docs_en, convertSpace, \"â€œ\")\n#docs_en <- tm_map(docs_en, convertSpace, \"â€\")\n#docs_en <- tm_map(docs_en, convertSpace, \"'\")\n\n#docs_en <- tm_map(docs_en, toSpace, \"`\")\n#docs_en <- tm_map(docs_en, toSpace, \" -\")\n\n#all_sample_corpus[[1]]$content[11]\n\n#Remove punctuation – replace punctuation marks with a space\nall_sample_corpus = tm_map(all_sample_corpus, removePunctuation)\nall_sample_corpus = tm_map(all_sample_corpus, content_transformer(tolower))\nall_sample_corpus = tm_map(all_sample_corpus, removeNumbers) \nall_sample_corpus = tm_map(all_sample_corpus, stripWhitespace)\n# \n# all_sample_corpus_test = tm_map(all_sample_corpus_test, removePunctuation)\n# all_sample_corpus_test = tm_map(all_sample_corpus_test, content_transformer(tolower))\n# all_sample_corpus_test = tm_map(all_sample_corpus_test, removeNumbers) \n# all_sample_corpus_test = tm_map(all_sample_corpus_test, stripWhitespace)\n\n#inspect the start of a particular document,\n# writeLines(as.character(all_sample_corpus[[1]]$content[1:5]))\n# writeLines(as.character(all_sample_corpus[[1]]$content[6:10]))\n# writeLines(as.character(all_sample_corpus[[1]]$content[11:15]))\n\n#docs_en = tm_map(docs_en, function(x) iconv(enc2utf8(x[[1]]$content), sub = \"byte\"))\n\n#Create a sub-samples of docs_en - blogs,\n\n#docs_en_blog_10pc = docs_en[[1]]$content[1:(floor(length(docs_en[[1]]$content)/10))]\n\n\n\n#Save the corpus\n#-------------------------------------\n\n#writeCorpus(all_sample_corpus, filenames=\"all_sample_corpus.txt\")\n#all_10pc <- readLines(\"all_sample_corpus.txt\")\n#all_sample_corpus = Corpus(VectorSource(list(all_10pc)))\n\n\n\n#DTM\n#------------------------------------\n\n# dtm <- DocumentTermMatrix(all_sample_corpus)\n# dtm\n# Terms(dtm)\n# \n# inspect(dtm[1,1000:1010])\n# \n# freq <- colSums(as.matrix(dtm))\n# length(freq)\n# ord <- order(freq,decreasing=TRUE)\n# freq[head(ord, 20)]\n# freq[tail(ord)]\n\n#length(findFreqTerms(dtm,lowfreq=80))\n\n#findAssocs(dtm,\"work\",0.6)\n\n#barplot(freq[head(ord, 10)])\n\n\n#Wordcloud\n#-------------------------------------------------\n\n#library(wordcloud)\n#set.seed(100)\n#wordcloud(names(freq),freq, min.freq=500,colors=brewer.pal(6,\"Dark2\"))\n\n\n#Ngrams (RWeka package)\n#-------------------------------------------------\n\nlibrary(RWeka)\n\nOnegramTokenizer = function(x) WordTokenizer(x)\ndtm <- DocumentTermMatrix(all_sample_corpus, control = list(tokenize = OnegramTokenizer, wordLengths = c(1, Inf)))\n\nTwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\ndtm2 <- DocumentTermMatrix(all_sample_corpus, control = list(tokenize = TwogramTokenizer))\n\nThreegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\ndtm3 <- DocumentTermMatrix(all_sample_corpus, control = list(tokenize = ThreegramTokenizer))\n\nFourgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))\ndtm4 <- DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FourgramTokenizer))\n\nFivegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))\ndtm5 <- DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FivegramTokenizer))\n\n\n\n# dtm = removeSparseTerms(dtm, 0.75)\n# dtm2 = removeSparseTerms(dtm2, 0.75)\n# dtm3 = removeSparseTerms(dtm3, 0.75)\n# dtm4 = removeSparseTerms(dtm4, 0.75)\n# dtm5 = removeSparseTerms(dtm5, 0.75)\n#inspect(dtm3_b[1,1000])\n\n# head(all_sample_corpus[[1]]$content,100)\n# \n# OnegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))\n# dtm_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = OnegramTokenizer))\n# \n# TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n# dtm2_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = TwogramTokenizer))\n# \n# ThreegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n# dtm3_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = ThreegramTokenizer))\n# \n# FourgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))\n# dtm4_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = FourgramTokenizer))\n# \n# FivegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))\n# dtm5_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = FivegramTokenizer))\n# \n# dtm_test = removeSparseTerms(dtm_test, 0.75)\n# dtm2_test = removeSparseTerms(dtm2_test, 0.75)\n# dtm3_test = removeSparseTerms(dtm3_test, 0.75)\n# dtm4_test = removeSparseTerms(dtm4_test, 0.75)\n# dtm5_test = removeSparseTerms(dtm5_test, 0.75)\n\n\n\n#Save and load\n#------------------------------------\n\n# save(dtm, file = \"dtm.RData\")\n# save(dtm2, file = \"dtm2.RData\")\n# save(dtm3, file = \"dtm3.RData\")\n# save(dtm4, file = \"dtm4.RData\")\n# save(dtm5, file = \"dtm5.RData\")\n# \n# save(dtm_test, file = \"dtm_test.RData\")\n# save(dtm2_test, file = \"dtm2_test.RData\")\n# save(dtm3_test, file = \"dtm3_test.RData\")\n# save(dtm4_test, file = \"dtm4_test.RData\")\n# save(dtm5_test, file = \"dtm5_test.RData\")\n# \n#save(all_sample_corpus, file = \"all_sample_corpus.RData\")\n#save(all_sample_corpus_test, file = \"all_sample_corpus_test\")\n\nload(file = \"dtm.RData\")\nload(file = \"dtm2.RData\")\nload(file = \"dtm3.RData\")\nload(file = \"dtm4.RData\")\nload(file = \"dtm5.RData\")\n\n# load(dtm_test, file = \"dtm_test.RData\")\n# load(dtm2_test, file = \"dtm2_test.RData\")\n# load(dtm3_test, file = \"dtm3_test.RData\")\n# load(dtm4_test, file = \"dtm4_test.RData\")\n# load(dtm5_test, file = \"dtm5_test.RData\")\n\nload(file = \"all_sample_corpus.RData\")\n#load(all_sample_corpus_test, file = \"all_sample_corpus_test\")\n\n\n\n\n\n#Probabilities\n#-----------------------------------\n\nfreq = colSums(as.matrix(dtm)) \ncounts_dtm = data.frame(Word=names(freq), Freq=freq) \ncounts_dtm = counts_dtm[counts_dtm$Freq>50,] #Pruning\n\nfreq2 = colSums(as.matrix(dtm2)) \ncounts_dtm2 = data.frame(Word=names(freq2), Freq=freq2) \ncounts_dtm2 = counts_dtm2[counts_dtm2$Freq>5,] #Pruning\n\nfreq3 = colSums(as.matrix(dtm3))\ncounts_dtm3 = data.frame(Word=names(freq3), Freq=freq3) \ncounts_dtm3 = counts_dtm3[counts_dtm3$Freq>1,] #Pruning\n\nfreq4 = colSums(as.matrix(dtm4)) \ncounts_dtm4 = data.frame(Word=names(freq4), Freq=freq4) \ncounts_dtm4 = counts_dtm4[counts_dtm4$Freq>2,] #Pruning\n\nfreq5 = colSums(as.matrix(dtm5)) \ncounts_dtm5 = data.frame(Word=names(freq5), Freq=freq5) \ncounts_dtm5 = counts_dtm5[counts_dtm5$Freq>2,] #Pruning\n\n\n#Drop levels,\ncounts_dtm$Word = droplevels(counts_dtm$Word)\ncounts_dtm2$Word = droplevels(counts_dtm2$Word)\ncounts_dtm3$Word = droplevels(counts_dtm3$Word)\ncounts_dtm4$Word = droplevels(counts_dtm4$Word)\ncounts_dtm5$Word = droplevels(counts_dtm5$Word)\n\nrm(dtm)\nrm(dtm2)\nrm(dtm3)\nrm(dtm4)\nrm(dtm5)\n\n\n#Maximum likelihood estimate for a bigram, P(Wi | Wi-1) = count(Wi-1, Wi) / count(Wi-1)\n#For example, P(Park | Car) = the number of bigrams of 'Car Park' divided by the number of unigrams of 'Car'\n\n#Unigram probabilities\nUni_length = sum(counts_dtm$Freq)\ncounts_dtm$Prob = counts_dtm$Freq / Uni_length\n\n\n\n\n#Bigram probabiltiies\ncounts_dtm2$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm2$Word)) {\n\n  uni_count = counts_dtm$Freq[counts_dtm$Word == (strsplit(as.character(counts_dtm2$Word[i]), \" \")[[1]][1])]\n  bi_count = counts_dtm2$Freq[i]\n  counts_dtm2$Prob[i] = bi_count / uni_count\n  i=i+1\n  print((i/length(counts_dtm2$Word)*100))\n  \n}\n\n\n\n#Trigram probabiltiies\n\n#Maximum likelihood estimate for a trigram, P(wi | wi-1 wi-2 ) = count(wi, wi-1, wi-2 ) / count(wi-1, wi-2 )\n#For example, P(Park | The car) = the number of trigrams of 'The car park' divided by the number of bigrams of 'The car'\n\n#https://gist.github.com/ttezel/4138642\n#https://class.coursera.org/nlp/lecture\n\ncounts_dtm3$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm3$Word)) {\n  \n  bi_count = counts_dtm2$Freq[counts_dtm2$Word == paste(strsplit(as.character(counts_dtm3$Word[i]), \" \")[[1]][1],\n                                                    strsplit(as.character(counts_dtm3$Word[i]), \" \")[[1]][2])]\n  tri_count = counts_dtm3$Freq[i]\n  counts_dtm3$Prob[i] = tri_count / bi_count\n  i=i+1\n  print((i/length(counts_dtm3$Word)*100))\n  \n}\n\n\n\n#Quadgram probabiltiies\n\ncounts_dtm4$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm4$Word)) {\n  \n  tri_count = counts_dtm3$Freq[counts_dtm3$Word == paste(strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][1],\n                                                        strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][2],\n                                                        strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][3])]\n  quad_count = counts_dtm4$Freq[i]\n  counts_dtm4$Prob[i] = quad_count / tri_count\n  i=i+1\n  print((i/length(counts_dtm4$Word)*100))\n  \n}\n\n\n\n\n\n#Fivegram probabiltiies\n\ncounts_dtm5$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm5$Word)) {\n  \n  quad_count = counts_dtm4$Freq[counts_dtm4$Word == paste(strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][1],\n                                                         strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][2],\n                                                         strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][3],\n                                                         strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][4])]\n  five_count = counts_dtm5$Freq[i]\n  counts_dtm5$Prob[i] = five_count / quad_count\n  i=i+1\n  print((i/length(counts_dtm5$Word)*100))\n  \n}\n\n\n\n\n#Simple interpolation (lambdas are static for all cases)\n#------------------------------------------------------------------\n\n#P(beer | case of) = l1 * P(beer | case of) + l2 * P(beer | of) + l3 * P(beer)\n\n#Replace low prob words with UNK? I think the KN deals with this\n\nsave(counts_dtm, file = \"counts_dtm.RData\")\nsave(counts_dtm2, file = \"counts_dtm2.RData\")\nsave(counts_dtm3, file = \"counts_dtm3.RData\")\nsave(counts_dtm4, file = \"counts_dtm4.RData\")\nsave(counts_dtm5, file = \"counts_dtm5.RData\")\n\nsave(freq, file = \"freq.RData\")\nsave(freq2, file = \"freq2.RData\")\nsave(freq3, file = \"freq3.RData\")\nsave(freq4, file = \"freq4.RData\")\nsave(freq5, file = \"freq5.RData\")\n\nload(file = \"counts_dtm.RData\")\nload(file = \"counts_dtm2.RData\")\nload(file = \"counts_dtm3.RData\")\nload(file = \"counts_dtm4.RData\")\nload(file = \"counts_dtm5.RData\")\n\nload(file = \"freq.RData\")\nload(file = \"freq2.RData\")\nload(file = \"freq3.RData\")\nload(file = \"freq4.RData\")\nload(file = \"freq5.RData\")\n\n\n\n\n  \n# Bi_length = length(counts_dtm2$Word)\n# Tri_length = length(counts_dtm3$Word)\n# Four_length = length(counts_dtm4$Word)\n# Five_length = length(counts_dtm5$Word)\n\n\n# counts_dtm2$Prob = counts_dtm2$Freq / Bi_length\n# counts_dtm3$Prob = counts_dtm3$Freq / Tri_length\n# counts_dtm4$Prob = counts_dtm4$Freq / Four_length\n# counts_dtm5$Prob = counts_dtm5$Freq / Five_length\n\n\n\n\n# freq_test = colSums(as.matrix(dtm_test)) \n# counts_dtm_test = data.frame(Word=names(freq_test), Freq=freq_test) \n# counts_dtm_test = counts_dtm_test[counts_dtm_test$Freq>100,]\n# \n# freq2_test = colSums(as.matrix(dtm2_test)) \n# counts_dtm2_test = data.frame(Word=names(freq2_test), Freq=freq2_test) \n# counts_dtm2_test = counts_dtm2_test[counts_dtm2_test$Freq>50,]\n# \n# freq3_test = colSums(as.matrix(dtm3_test))\n# counts_dtm3_test = data.frame(Word=names(freq3_test), Freq=freq3_test) \n# counts_dtm3_test = counts_dtm3_test[counts_dtm3_test$Freq>5,]\n# \n# freq4_test = colSums(as.matrix(dtm4_test)) \n# counts_dtm4_test = data.frame(Word=names(freq4_test), Freq=freq4_test) \n# counts_dtm4_test = counts_dtm4_test[counts_dtm4_test$Freq>2,]\n# \n# freq5_test = colSums(as.matrix(dtm5_test)) \n# counts_dtm5_test = data.frame(Word=names(freq5_test), Freq=freq5_test) \n# counts_dtm5_test = counts_dtm5_test[counts_dtm5_test$Freq>2,]\n\n\n\n\n\n\n\n\n\n#Kneser-Ney smoothing\n#---------------------------------------------\n\n#http://www-rohan.sdsu.edu/~gawron/compling/course_core/lectures/kneser_ney.pdf\n\n#First, we need the corpus as bigrams (not just the unique bigrams, but the whole lot),\n\nbis = as.data.frame(strsplit(as.character(counts_dtm2$Word), \" \"))\nbis = as.data.frame(t(bis))\nbis = cbind(bis, counts_dtm2$Freq)\ncolnames(bis) = c(\"Word1\", \"Word2\", \"Freq\")\n\n\n\n#Next, we need to know what are the most common 2nd words in the bigrams\n\nmost_bi = sort(bis$Freq, decreasing = T)\nmost_bis = bis[bis$Freq %in% most_bi,]\n\nmost_bis = most_bis[order(-most_bis$Freq),]\n\nmost_bis = droplevels(most_bis)\n\nunique_w2 = unique(most_bis$Word2)\nunique_w1 = unique(most_bis$Word1)\n\nlength_element_w1 = length(unique_w1)\n\n#Then, from these common endings, we need to know how many bigrams each completes\n#Finally, we work out the continuum probability based upon this\n\nkn = data.frame()\ni=1\n\nwhile (i <= length(unique_w2)) {\n  \n  sum_element = sum(most_bis$Freq[most_bis$Word2 == unique_w2[i]])\n  length_element_w2 = length(most_bis$Word1[most_bis$Word2 == unique_w2[i]])\n  prob_element = length_element_w2 / length_element_w1 #No. of word types seen to precede / no. of words preceding all words\n  kn[i,1] = unique_w2[i]\n  kn[i,2] = sum_element\n  kn[i,3] = length_element_w2\n  kn[i,4] = prob_element\n  i=i+1\n  \n}\n\ncolnames(kn) = c(\"2nd Word\", \"Freq\", \"No. of bigrams it completes\", \"Pcont\")\n\nkn = kn[order(-kn$Pcont),]\n\n#save(kn, file = \"kn.RData\")\nload(file = \"kn.RData\")\n\nunknown1 = as.character(kn[1,1])\nunknown2 = as.character(kn[2,1])\nunknown3 = as.character(kn[3,1])\n\nunknowns = data.frame(unknown1,unknown2,unknown3)\n\n\n\n#Prediction function\n#-------------------------------------\n\n\n\nNWP = function(new_word, unknowns) {\n  \n  #new_word_full = 'least I'\n  \n  new_word_full = new_word\n  \n  #new_word_full = \"you get\" #Length is less than 5, so no need to split\n  new_word_full = tolower(new_word_full)\n  new_word_full = removePunctuation(new_word_full)\n  new_word_full = removeNumbers(new_word_full)\n  new_word_full = stripWhitespace(new_word_full)\n  new_word_full = str_trim(new_word_full)\n  \n  new_word_full_original = new_word_full\n  \n  new_word_split = strsplit(new_word_full, \" \")\n  \n  len_new_word = length(new_word_split[[1]])\n  \n  i=1\n  \n  \n  if (length(new_word_split[[1]]) > 4) {\n    \n    new_word_full = paste(new_word_split[[1]][len_new_word-3],new_word_split[[1]][len_new_word-2],\n                          new_word_split[[1]][len_new_word-1], new_word_split[[1]][len_new_word])\n    \n    new_word_split = strsplit(new_word_full, \" \")\n    \n  } \n  \n  no_match = 0\n  \n  \n  \n  #5GRAM CHECKER\n  ---------------------\n    \n    if(length(new_word_split[[1]]) <= 4) {\n      \n      if(length(new_word_split[[1]]) == 4) {\n        \n        Five_matches = as.character(counts_dtm5$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        Five_matches_f = as.numeric(counts_dtm5$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        \n        if (length(Five_matches) == 0 | (sum(Five_matches_f) / length(Five_matches_f)) < 2) {\n          \n          #print(\"No match in the 5grams. Let's keep checking...\")\n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3],new_word_split[[1]][4])\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Five_split = strsplit(Five_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Five_split)) {\n            \n            Token[i] = as.list(paste(Five_split[[1]][1], Five_split[[1]][2], Five_split[[1]][3], Five_split[[1]][4]))\n            Token[[i]][2] = Five_split[[i]][5]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Five_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Five_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Five_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred = data.frame()\n          \n          while (i <= x) {\n            \n            pred[i,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred[i,2] = markov[i,2]\n            pred[i,3] = counts_dtm5$Prob[counts_dtm5 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred) = c(\"Fivegram Prediction\", \"Freq\", \"Prob\")\n          pred$Prob = round(pred$Prob, 3)\n          \n        }\n      }\n      \n      \n      if(length(new_word_split[[1]]) == 3) { #QUADGRAM CHECKER\n        \n        Quad_matches = as.character(counts_dtm4$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        Quad_matches_f = as.numeric(counts_dtm4$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        \n        if (length(Quad_matches) == 0 | (sum(Quad_matches_f) / length(Quad_matches_f)) < 2) {\n          \n          #print(\"No match in the 4grams. Let's keep checking...\")\n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3])\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Quad_split = strsplit(Quad_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Quad_split)) {\n            \n            Token[i] = as.list(paste(Quad_split[[1]][1], Quad_split[[1]][2], Quad_split[[1]][3]))\n            Token[[i]][2] = Quad_split[[i]][4]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Quad_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Quad_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Quad_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred = data.frame()\n          \n          while (i <= x) {\n            \n            pred[i,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred[i,2] = markov[i,2]\n            pred[i,3] = counts_dtm4$Prob[counts_dtm4 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred) = c(\"Quadgram Prediction\", \"Freq\", \"Prob\")\n          pred$Prob = round(pred$Prob, 3)\n          \n        }\n      }\n      \n      \n      if(length(new_word_split[[1]]) == 2) { #TRIGRAM CHECKER\n        \n        Tri_matches = as.character(counts_dtm3$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        Tri_matches_f = as.numeric(counts_dtm3$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        \n        \n        if (length(Tri_matches) == 0 | (sum(Tri_matches_f) / length(Tri_matches_f)) < 2) {\n          \n          #print(\"No match in the 3grams. Let's keep checking...\")\n          new_word_full = new_word_split[[1]][2]\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Tri_split = strsplit(Tri_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Tri_split)) {\n            \n            Token[i] = as.list(paste(Tri_split[[1]][1], Tri_split[[1]][2]))\n            Token[[i]][2] = Tri_split[[i]][3]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Tri_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Tri_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Tri_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred = data.frame()\n          \n          while (i <= x) {\n            \n            pred[i,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred[i,2] = markov[i,2]\n            pred[i,3] = counts_dtm3$Prob[counts_dtm3 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred) = c(\"Trigram Prediction\", \"Freq\", \"Prob\")\n          pred$Prob = round(pred$Prob, 3)\n          \n        }\n      }\n      \n      \n      \n      if(length(new_word_split[[1]]) == 1) { #BIGRAM CHECKER\n        \n        Bi_matches = as.character(counts_dtm2$Word[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        Bi_matches_f = as.numeric(counts_dtm2$Freq[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        \n        if (length(Bi_matches) == 0) {\n          \n          #print(\"No match in the 2grams. Let's keep checking...\")\n          no_match = 1\n          \n        } \n        \n        else {\n          \n          Bi_split = strsplit(Bi_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Bi_split)) {\n            \n            Token[i] = as.list(Bi_split[[1]][1])\n            Token[[i]][2] = Bi_split[[i]][2]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Bi_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Bi_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Bi_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred = data.frame()\n          \n          while (i <= x) {\n            \n            pred[i,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred[i,2] = markov[i,2]\n            pred[i,3] = counts_dtm2$Prob[counts_dtm2 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred) = c(\"Bigram Prediction\", \"Freq\", \"Prob\")\n          pred$Prob = round(pred$Prob, 3)\n          \n        }\n        \n      }\n      \n      \n      \n      #No match from a single word!\n      #---------------------------------\n      \n      if (no_match == 1) {\n        \n        #pred = data.frame()  \n        #pred[1,1] = \"Prediction: 123\"\n        \n        if (new_word_split %in% dict) {\n          \n          #       max = head(sort(counts_dtm$Freq, decreasing = T),1)\n          #       max_w = counts_dtm[counts_dtm$Freq == max,]\n          #       max_w = as.character(max_w$Word)\n          \n          pred = data.frame()\n          pred[1,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(unknowns[1,1])))\n          pred[2,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(unknowns[1,2])))\n          pred[3,1] = (paste(\"Prediction\",i,\":\", new_word_full_original, \"|\", as.character(unknowns[1,3])))\n          colnames(pred) = c(\"Unigram Prediction\", \"Probability\")\n          pred$Probability = round(pred$Probability, 3)\n          \n        } else {\n          \n          nearest = amatch(new_word_split, counts_dtm$Word, maxDist = 5, method = \"lcs\")\n          \n          pred = data.frame()\n          pred[i,1] = paste(\"Word not found. Did you mean '\", counts_dtm$Word[nearest], \"' ?\", sep = \"\")\n          \n        }\n        \n      } \n      \n      return(pred)\n      \n    }\n}\n\n\n\n\n\n\n#Individual prediction\n#-----------------------------------------\n\n\n#Do I have to be a bit clevered than just 'if there are less than 2 go to lower-gram' and actually work out Ps?\n\n\ntest_word = \"a case of\"\n\ncounter = 0\neval = data.frame()\n\ntest = test_word\ntest_split = strsplit(test, \" \")\nlen_new_word = length(test_split[[1]])\n\n\nif (length(test_split[[1]]) > 4) {\n  \n  test = paste(test_split[[1]][len_new_word-3],test_split[[1]][len_new_word-2],\n               test_split[[1]][len_new_word-1], test_split[[1]][len_new_word])\n  \n  test_split = strsplit(test, \" \")\n  \n} \n\n\n#if (length(test_split[[1]]) > 1) {test_split[[1]] = test_split[[1]][1:length(test_split[[1]])-1]}\n\nif (length(test_split[[1]]) == 4) {test_func=paste(test_split[[1]][1],test_split[[1]][2],test_split[[1]][3],test_split[[1]][4])}\nif (length(test_split[[1]]) == 3) {test_func=paste(test_split[[1]][1],test_split[[1]][2],test_split[[1]][3])}\nif (length(test_split[[1]]) == 2) {test_func=paste(test_split[[1]][1],test_split[[1]][2])}\nif (length(test_split[[1]]) == 1) {test_func=test_split[[1]][1]}\n\np_return = NWP(test_func, unknowns)\n\np_return\n\n#Adjust the probabilities \n\nl1 = 1/3\nl2 = 1/3\nl3 = 1/3\n\ni=1\n\nwhile (i <= length(p_return$`Trigram Prediction`)) {\n\n  split = strsplit(p_return$`Trigram Prediction`[i], \" \")\n  \n  p_return$Adjust_prob[i] =  round(l1*(p_return$Prob[i]) + \n    l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n    l3*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n\n  i=i+1\n  \n}\n\np_return = p_return[order(-p_return$Adjust_prob),]\np_return\n\n\n\n\nbeer5 = counts_dtm5$Freq[counts_dtm5$Word == 'and a pack of the']\nbeer4 = counts_dtm4$Freq[counts_dtm4$Word == 'a pack of beer']\nbeer3 = counts_dtm3$Freq[counts_dtm3$Word == 'pack of beer']\nbeer2 = counts_dtm2$Freq[counts_dtm2$Word == 'of beer']\nbeer1 = counts_dtm$Freq[counts_dtm$Word == 'beer']\n\nl1*(beer1 / sum(counts_dtm$Freq)) + l2*(beer2 / beer1) + l3*(beer3 / beer2)\n\n\n\nthe5 = counts_dtm5$Freq[counts_dtm5$Word == 'and a pack of the']\nthe4 = counts_dtm4$Freq[counts_dtm4$Word == 'a pack of the']\nthe3 = counts_dtm3$Freq[counts_dtm3$Word == 'pack of the']\nthe2 = counts_dtm2$Freq[counts_dtm2$Word == 'of the']\nthe1 = counts_dtm$Freq[counts_dtm$Word == 'the']\n\nl1*(the1 / sum(counts_dtm$Freq)) + l2*(the2 / the1) + l3*(the3 / the2)\n\n\ncounts_dtm4$Prob[counts_dtm4$Word == 'a pack of cards']\ncounts_dtm3$Freq[counts_dtm3$Word == 'a pack of']\n\n\n#Evaluation\n#-----------------------------------------\n\ntest_set_2 = as.character(sample(counts_dtm2_test$Word, size = 100))\ntest_set_3 = as.character(sample(counts_dtm3_test$Word, size = 100))\ntest_set_4 = as.character(sample(counts_dtm4_test$Word, size = 100))\ntest_set_5 = as.character(sample(counts_dtm5_test$Word, size = 100))\n\ntest_set = c(test_set_3,test_set_4,test_set_5)\n\n\ni = 1\ncounter = 0\neval = data.frame()\n\nwhile (i <= length(test_set)) {\n  \n  test = test_set[i]\n  test_split = strsplit(test, \" \")\n  len_new_word = length(test_split[[1]])\n  \n  \n  if (length(test_split[[1]]) > 4) {\n    \n    test = paste(test_split[[1]][len_new_word-3],test_split[[1]][len_new_word-2],\n                 test_split[[1]][len_new_word-1], test_split[[1]][len_new_word])\n    \n    test_split = strsplit(test, \" \")\n    \n  } \n  \n  \n  test_split[[1]] = test_split[[1]][1:length(test_split[[1]])-1]\n  \n  if (length(test_split[[1]]) == 4) {test_func=paste(test_split[[1]][1],test_split[[1]][2],test_split[[1]][3],test_split[[1]][4])}\n  if (length(test_split[[1]]) == 3) {test_func=paste(test_split[[1]][1],test_split[[1]][2],test_split[[1]][3])}\n  if (length(test_split[[1]]) == 2) {test_func=paste(test_split[[1]][1],test_split[[1]][2])}\n  if (length(test_split[[1]]) == 1) {test_func=test_split[[1]][1]}\n  \n  p_return = NWP(test_func)\n  \n  p1 = p_return[1,1]\n  p1 = strsplit(p1, \": \")\n  p1 = p1[[1]][2]\n  \n  if (dim(p_return)[1] == 2) {\n    \n    p2 = p_return[2,1]\n    p2 = strsplit(p2, \": \")\n    p2 = p2[[1]][2]\n  } else {p2 = \" \"}\n  \n  if (dim(p_return)[1] == 3) {\n    \n    p3 = p_return[2,1]\n    p3 = strsplit(p3, \": \")\n    p3 = p3[[1]][2]\n  } else {p3 = \" \"}\n  \n  \n  eval[i,1] = test\n  eval[i,2] = p1\n  eval[i,3] = p2\n  eval[i,4] = p3\n  \n  if (test == eval[i,2] | test == eval[i,3] | test == eval[i,4]) {counter = counter+1}\n  \n  i=i+1\n  print(i/length(test_set)*100)\n  \n}\n\n\naccuracy = counter / length(test_set)\naccuracy*100\n\neval\n\n\n#Latest (18/03/16): 5% used. Accuracy ~ 42% Look into the 5grams and the test vs pred bit towards the end. Not quite right \n\n\n\n\n\n#Quiz 2 hack\n#----------------------------\n\ncounts_dtm2[grep(\"^of\\\\b\", counts_dtm2$Word),]\ncounts_dtm3[grep(\"^case of\", counts_dtm3$Word),]\ncounts_dtm4[grep(\"^a case of\", counts_dtm4$Word),]\ncounts_dtm5[grep(\"^and a case of\", counts_dtm5$Word),]\n\n#and a case of mountain x 1\n\n#a case of beer x 1\n#a case of the x 10\n#a case of mistaken x 2\n#a case of waiting x 2\n\n#case of a x25\n#case of beer x1\n\n\n\n\n\n\"The guy in front of me just bought a pound of bacon, a bouquet, and a case of\"\n\n#Algorithm:     the\n#Grep looking:  beer\n#3rd:           beer (correct)\n\n\n\"You're the reason why I smile everyday. Can you follow me please? It would mean the\"\n\n#Algorithm:     world\n#Grep looking:  world \n#3rd:           world (correct)\n\n\n\"Hey sunshine, can you follow me and make me the\"\n\n#Algorithm:     most\n#Grep looking:  ? \n#3rd:           happiest (correct)\n\n\n\"Very early observations on the Bills game: Offense still struggling but the\"\n\n#Algorithm:     best\n#Grep looking:  ? (correct = defence)\n#3rd:           best\n\n\n\"Go on a romantic date at the\"\n\n#Algorithm:     beverly \n#Grep looking:  ? (don't know, movies was wrong)\n#3rd:           end\n\n\n\"Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my\"\n\n#Algorithm:     way \n#Grep looking:  way \n#3rd:           way (correct)\n\n\n\"Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some\"\n\n#Algorithm:     time \n#Grep looking:  time \n#3rd:           time (correct)\n\n\n\"After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little\"\n\n#Algorithm:     brother\n#Grep looking:  ? (correct = fingers)\n#3rd:           brother\n\n\n\"Be grateful for the good times and keep the faith during the\"\n\n#Algorithm:     day\n#Grep looking:  ? (correct = bad)\n#3rd:           day\n\n\n\"If this isn't the cutest thing you've ever seen, then you must be\"\n\n\n#Algorithm:     a \n#Grep looking:   (correct = insane)\n#3rd:           a\n\n#-------------------------------------------------------------------\n",
    "created" : 1457604477416.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1016506972",
    "id" : "21DC84BB",
    "lastKnownWriteTime" : 1458832320,
    "path" : "C:/Users/rob.harrand/Desktop/WORK/Coursera/Module 10 - Capstone/Capstone.R",
    "project_path" : "Capstone.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}