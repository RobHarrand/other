{
    "contents" : "############################\n#R code for Data Science Specialisation Capstone - server.R\n#Completed 19/04/16\n#Created by RH\n############################\n\n#Load libraries,\n\nlibrary(\"tm\")\nlibrary(\"SnowballC\")\nlibrary(\"stringr\")\nlibrary(\"readr\")\nlibrary('stringdist')\nlibrary(wordcloud)\n\n#Load files,\n#-------------------------------------------\n\nload(file = \"lambdas_chosen.RData\") #Load the lambda values\n\nl1 = lambdas_chosen[1]\nl2 = lambdas_chosen[2]\nl3 = lambdas_chosen[3]\nl4 = lambdas_chosen[4]\nl5 = lambdas_chosen[5]\n\ndict = read_lines(\"texts/Dict.txt\") #Load the dictionary file\n\nload(file = \"counts_dtm_2pc.RData\")\nload(file = \"counts_dtm2_2pc.RData\")\nload(file = \"counts_dtm3_2pc.RData\")\nload(file = \"counts_dtm4_2pc.RData\")\nload(file = \"counts_dtm5_2pc.RData\")\n\nload(file = \"kn_2pc.RData\") #Load Kneser-Ney data\n\nunknown1 = as.character(kn[1,1])\nunknown2 = as.character(kn[2,1])\nunknown3 = as.character(kn[3,1])\n\nunknowns = data.frame(unknown1,unknown2,unknown3)\nrm(kn)\n\n\n#Prediction function\n#-------------------------------------\n\nNWP = function(new_word, unknowns) {\n\n  new_word_full = new_word\n  \n  #Clean and format the user-entered word,\n  new_word_full = tolower(new_word_full)\n  new_word_full = removePunctuation(new_word_full)\n  new_word_full = removeNumbers(new_word_full)\n  new_word_full = stripWhitespace(new_word_full)\n  new_word_full = str_trim(new_word_full)\n  \n  new_word_full_original = new_word_full\n  \n  new_word_split = strsplit(new_word_full, \" \")\n  \n  len_new_word = length(new_word_split[[1]])\n  \n  i=1\n  \n  \n  if (length(new_word_split[[1]]) > 4) {\n    \n    new_word_full = paste(new_word_split[[1]][len_new_word-3],new_word_split[[1]][len_new_word-2],\n                          new_word_split[[1]][len_new_word-1], new_word_split[[1]][len_new_word])\n    \n    new_word_split = strsplit(new_word_full, \" \")\n    \n  } \n  \n  no_match = 0\n  \n  \n  #5GRAM CHECKER\n  ---------------------\n    \n    if(length(new_word_split[[1]]) <= 4) {\n      \n      if(length(new_word_split[[1]]) == 4) {\n        \n        Five_matches = as.character(counts_dtm5$Word[grep(paste(\"^\", new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        Five_matches_f = as.numeric(counts_dtm5$Freq[grep(paste(\"^\", new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        \n        if (length(Five_matches) == 0) {\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3],new_word_split[[1]][4])\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Five_split = strsplit(Five_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Five_split)) {\n            \n            Token[i] = as.list(paste(Five_split[[1]][1], Five_split[[1]][2], Five_split[[1]][3], Five_split[[1]][4]))\n            Token[[i]][2] = Five_split[[i]][5]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Five_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Five_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Five_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred5 = data.frame()\n          \n          while (i <= x) {\n            \n            pred5[i,1] = (paste(\"(Fivegram) Prediction\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred5[i,2] = markov[i,2]\n            pred5[i,3] = counts_dtm5$Prob[counts_dtm5$Word == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred5) = c(\"Fivegram Prediction\", \"Freq\", \"Prob\")\n          pred5$Prob = round(pred5$Prob, 3)\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3],new_word_split[[1]][4])\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        }\n      }\n      \n      \n      \n      if(length(new_word_split[[1]]) == 3) { #QUADGRAM CHECKER   \n        \n        Quad_matches = as.character(counts_dtm4$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        Quad_matches_f = as.numeric(counts_dtm4$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        \n        if (length(Quad_matches) == 0 | (sum(Quad_matches_f) / length(Quad_matches_f)) < 2) {\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3])\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Quad_split = strsplit(Quad_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Quad_split)) {\n            \n            Token[i] = as.list(paste(Quad_split[[1]][1], Quad_split[[1]][2], Quad_split[[1]][3]))\n            Token[[i]][2] = Quad_split[[i]][4]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Quad_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Quad_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Quad_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred4 = data.frame()\n          \n          while (i <= x) {\n            \n            pred4[i,1] = (paste(\"(Quadgram) Prediction:\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred4[i,2] = markov[i,2]\n            pred4[i,3] = counts_dtm4$Prob[counts_dtm4$Word == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred4) = c(\"Quadgram Prediction\", \"Freq\", \"Prob\")\n          pred4$Prob = round(pred4$Prob, 3)\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3])\n          new_word_split = strsplit(new_word_full, \" \")\n        }\n        \n      }\n      \n      \n      \n      if(length(new_word_split[[1]]) == 2) { #TRIGRAM CHECKER  \n        \n        Tri_matches = as.character(counts_dtm3$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        Tri_matches_f = as.numeric(counts_dtm3$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        \n        \n        if (length(Tri_matches) == 0 | (sum(Tri_matches_f) / length(Tri_matches_f)) < 2) {\n          \n          new_word_full = new_word_split[[1]][2]\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        } \n        \n        else {\n          \n          Tri_split = strsplit(Tri_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Tri_split)) {\n            \n            Token[i] = as.list(paste(Tri_split[[1]][1], Tri_split[[1]][2]))\n            Token[[i]][2] = Tri_split[[i]][3]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Tri_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Tri_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Tri_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred3 = data.frame()\n          \n          while (i <= x) {\n            \n            pred3[i,1] = (paste(\"(Trigram) Prediction:\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred3[i,2] = markov[i,2]\n            pred3[i,3] = counts_dtm3$Prob[counts_dtm3 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred3) = c(\"Trigram Prediction\", \"Freq\", \"Prob\")\n          pred3$Prob = round(pred3$Prob, 3)\n          \n          new_word_full = new_word_split[[1]][2]\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        }\n      }\n      \n      \n      \n      if(length(new_word_split[[1]]) == 1) { #BIGRAM CHECKER      \n        \n        Bi_matches = as.character(counts_dtm2$Word[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        Bi_matches_f = as.numeric(counts_dtm2$Freq[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        \n        if (length(Bi_matches) == 0) {\n          \n          no_match = 1\n          \n        } \n        \n        else {\n          \n          Bi_split = strsplit(Bi_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Bi_split)) {\n            \n            Token[i] = as.list(Bi_split[[1]][1])\n            Token[[i]][2] = Bi_split[[i]][2]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Bi_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Bi_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Bi_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred2 = data.frame()\n          \n          while (i <= x) {\n            \n            pred2[i,1] = (paste(\"(Bigram) Prediction:\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred2[i,2] = markov[i,2]\n            pred2[i,3] = counts_dtm2$Prob[counts_dtm2 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred2) = c(\"Bigram Prediction\", \"Freq\", \"Prob\")\n          pred2$Prob = round(pred2$Prob, 3)\n          no_match = 1\n          \n        }\n      }\n      \n      \n      \n      #No match from a single word\n      #---------------------------------\n      \n      if (no_match == 1) {\n        \n        if (new_word_split %in% dict) {\n          \n          pred1 = data.frame()\n          pred1[1,1] = (paste(\"(Unigram) Prediction:\", new_word_full_original, \"|\", as.character(unknowns[1,1])))\n          pred1[2,1] = (paste(\"(Unigram) Prediction:\", new_word_full_original, \"|\", as.character(unknowns[1,2])))\n          pred1[3,1] = (paste(\"(Unigram) Prediction:\", new_word_full_original, \"|\", as.character(unknowns[1,3])))\n          pred1[1,2] = 1\n          pred1[1,3] = 0.001\n          pred1[2,2] = 1\n          pred1[2,3] = 0.001\n          pred1[3,2] = 1\n          pred1[3,3] = 0.001\n          colnames(pred1) = c(\"Unigram Prediction\", \"Freq\", \"Prob\")\n\n        } else {\n          \n          nearest = amatch(new_word_split, counts_dtm$Word[counts_dtm$Word != new_word_split], maxDist = 5, method = \"lcs\")\n          \n          pred1 = data.frame()\n          pred1[1,1] = paste(\"Word not found in dictionary. Did you mean '\", counts_dtm$Word[nearest], \"' ?\", sep = \"\")\n          pred1[1,2] = \"2\"\n          pred1[1,3] = \"2\"\n          \n        }\n        \n      } \n      \n      if (!exists(\"pred5\")) {pred5 = data.frame(\"No Fivegram match\", \" \", \" \")}\n      if (!exists(\"pred4\")) {pred4 = data.frame(\"No Quadgram match\", \" \", \" \")}\n      if (!exists(\"pred3\")) {pred3 = data.frame(\"No Trigram match\", \" \", \" \")}\n      if (!exists(\"pred2\")) {pred2 = data.frame(\"No Bigram match\", \" \", \" \")}\n      if (!exists(\"pred1\")) {pred1 = data.frame(\"No Unigram match\", \" \", \" \")}\n      \n      colnames(pred5) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred4) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred3) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred2) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred1) = c(\"Prediction\", \"Freq\", \"Prob\")\n      \n      pred5[,1] = as.character(pred5[,1])\n      pred5$Freq = as.numeric(pred5$Freq)\n      pred5$Prob = as.numeric(pred5$Prob)\n      \n      pred4[,1] = as.character(pred4[,1])\n      pred4$Freq = as.numeric(pred4$Freq)\n      pred4$Prob = as.numeric(pred4$Prob)\n      \n      pred3[,1] = as.character(pred3[,1])\n      pred3$Freq = as.numeric(pred3$Freq)\n      pred3$Prob = as.numeric(pred3$Prob)\n      \n      pred2[,1] = as.character(pred2[,1])\n      pred2$Freq = as.numeric(pred2$Freq)\n      pred2$Prob = as.numeric(pred2$Prob)\n      \n      pred1[,1] = as.character(pred1[,1])\n      pred1$Freq = as.numeric(pred1$Freq)\n      pred1$Prob = as.numeric(pred1$Prob)\n      \n      pred_all = rbind(pred5, pred4, pred3, pred2, pred1)\n      \n      return(pred_all)\n      \n    }\n}\n\n\n\n#Shiny server\n#-----------------------------\n\nshinyServer(\n    function(input, output, session) {\n        \n        observeEvent(input$Predict, {\n            \n            test_word = input$Sentence\n            save(test_word, file = \"Sentence.RData\")\n            \n            if (test_word == \"\" | test_word == \" \") {\n              \n              output$text1 <- renderText(\"Please enter a word\")\n              \n            } else {\n            \n            p_return = NWP(test_word, unknowns) \n            \n            prediction5 = p_return[grep(\"Five\", p_return$Prediction),]\n            prediction4 = p_return[grep(\"Quad\", p_return$Prediction),]\n            prediction3 = p_return[grep(\"Tri\", p_return$Prediction),]\n            prediction2 = p_return[grep(\"Bi\", p_return$Prediction),]\n\n            if (length(grep(\"Uni\", p_return$Prediction)) != 0) {\n              \n              prediction1 = p_return[grep(\"Uni\", p_return$Prediction),]\n              \n            } else {prediction1 = p_return[grep(\"not found\", p_return$Prediction),]}\n\n\n            \n            #Adjust Fivegrams\n            #----------------\n            \n            i=1\n            \n            if (prediction5$Prediction[1] != 'No Fivegram match') {\n              \n              while (i <= length(prediction5$Prediction)) {\n                \n                split = strsplit(prediction4$Prediction[i], \" \")\n                \n                prediction5$Adjust_prob[i] =  round(l5*(prediction5$Prob[i]) + \n                                                      l4*(counts_dtm4$Prob[counts_dtm4$Word == paste(split[[1]][length(split[[1]])-4],split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                                      l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                                      l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                                      l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n                \n                i=i+1\n              }\n              \n            } else {prediction5$Adjust_prob[1] = 0}\n            \n            \n            #Adjust Fourgrams\n            #----------------\n            \n            i=1\n            \n            if (prediction4$Prediction[1] != 'No Quadgram match') {\n              \n              while (i <= length(prediction4$Prediction)) {\n                \n                split = strsplit(prediction4$Prediction[i], \" \")\n                \n                prediction4$Adjust_prob[i] =  round(l4*(prediction4$Prob[i]) +\n                                                      l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                                      l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                                      l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n                \n                i=i+1\n              }\n              \n            } else {prediction4$Adjust_prob[1] = 0}\n            \n            \n            #Adjust Trigrams\n            #----------------\n            \n            i=1\n            \n            if (prediction3$Prediction[1] != 'No Trigram match') {\n              \n              while (i <= length(prediction3$Prediction)) {\n                \n                split = strsplit(prediction3$Prediction[i], \" \")\n                \n                prediction3$Adjust_prob[i] =  round(l3*(prediction3$Prob[i]) + \n                                                      l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                                      l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n                \n                i=i+1\n              }\n            }  else {prediction3$Adjust_prob[1] = 0}\n            \n            \n            #Adjust Bigrams\n            #----------------\n            \n            i=1\n            \n            if (prediction2$Prediction[1] != 'No Bigram match') {\n              \n              while (i <= length(prediction2$Prediction)) {\n                \n                split = strsplit(prediction2$Prediction[i], \" \")\n                \n                prediction2$Adjust_prob[i] =  round(l2*(prediction2$Prob[i]) + \n                                                      l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n                \n                i=i+1\n              } \n            } else {prediction2$Adjust_prob[1] = 0}\n            \n            \n            #Adjust Unigrams\n            #----------------\n            \n            i=1\n            \n            while (i <= length(prediction1$Prediction)) {\n              \n              prediction1$Adjust_prob[i] =  l1*(prediction1$Prob[i])\n              \n              i=i+1\n            }\n            \n            \n            #Re-merge,\n            \n            pred_all = rbind(prediction5, \n                             prediction4, \n                             prediction3, \n                             prediction2, \n                             prediction1)\n            \n            pred_all$Adjust_prob = as.numeric(pred_all$Adjust_prob)\n            pred_all = pred_all[order(-pred_all$Adjust_prob),]\n            \n            pred_all_split = strsplit(pred_all$Prediction, \" | \")\n            \n            #Make the top 3 results unique,\n            \n            if (pred_all_split[[1]][length(pred_all_split[[1]])] == pred_all_split[[2]][length(pred_all_split[[2]])]) {\n              \n              pred_all = pred_all[-2,]\n              \n            }\n            \n            pred_all_split = strsplit(pred_all$Prediction, \" | \")\n            \n            if (pred_all_split[[2]][length(pred_all_split[[2]])] == pred_all_split[[3]][length(pred_all_split[[3]])]) {\n              \n              pred_all = pred_all[-3,]\n              \n            }\n            \n            \n            pred_output1 = pred_all$Prediction[1]\n            pred_output2 = pred_all$Prediction[2]\n            pred_output3 = pred_all$Prediction[3]\n\n            output$text1 <- renderText({{pred_output1}})\n            output$text2 <- renderText({{pred_output2}})\n            output$text3 <- renderText({{pred_output3}})\n            \n            save(pred_output1, file = \"pred_output1.RData\")\n            save(pred_output2, file = \"pred_output2.RData\")\n            save(pred_output3, file = \"pred_output3.RData\")\n            \n            \n            #Wordcloud\n            #-------------------------------------------------\n            \n            set.seed(100)\n            \n            pred_word = pred_all[(!grepl(\"No Fivegram|No Quadgram|No Trigram|No Bigram|Word not found\", pred_all$Prediction)),]\n            pred_words = strsplit(pred_word$Prediction, \" | \")\n            pred_words = pred_words\n            \n            if (length(pred_words) != 0) {\n            \n              i=1\n              pred_wordcloud = data.frame()\n              \n              while (i <= length(pred_words)) {\n                \n                pred_wordcloud[i,1] = pred_words[[i]][length(pred_words[[i]])]\n                pred_wordcloud[i,2] = pred_words[[i]][length(pred_words[[i]])]\n                i=i+1\n                \n              }\n              \n              pred_wordcloud$Freq = pred_word$Adjust_prob*1000\n              colnames(pred_wordcloud) = c(\"Word1\", \"Word2\", \"Freq\")\n              \n              if (nrow(pred_word) >3) {\n                \n                # Make the wordcloud drawing predictable during a session\n                wordcloud_rep <- repeatable(wordcloud)\n                \n                output$plot <- renderPlot({\n                  wordcloud_rep(pred_wordcloud$Word1,pred_wordcloud$Freq, min.freq=1,colors=brewer.pal(6,\"Dark2\"))\n                }) \n                \n              }\n            }\n           }\n        })\n        \n      observeEvent(input$Update1, {\n        load(\"pred_output1.RData\")\n        u1 = strsplit(pred_output1, \" | \")\n        u1 = u1[[1]][length(u1[[1]])]\n        u1 = paste(input$Sentence, u1)\n        updateTextInput(session, \"Sentence\", value = u1)\n        })\n        \n        observeEvent(input$Update2, {\n          load(\"pred_output2.RData\")\n          u2 = strsplit(pred_output2, \" | \")\n          u2 = u2[[1]][length(u2[[1]])]\n          u2 = paste(input$Sentence, u2)\n          updateTextInput(session, \"Sentence\", value = u2)\n        })\n        \n        observeEvent(input$Update3, {\n          load(\"pred_output3.RData\")\n          u3 = strsplit(pred_output3, \" | \")\n          u3 = u3[[1]][length(u3[[1]])]\n          u3 = paste(input$Sentence, u3)\n          updateTextInput(session, \"Sentence\", value = u3)\n        })\n        \n        observeEvent(input$Alt, {\n          load(\"pred_output1.RData\")\n          u1 = strsplit(pred_output1, \"'\")\n          u1 = u1[[1]][2]\n          input_s = input$Sentence\n          input_s = strsplit(input_s, \" \")\n          input_s = head(input_s[[1]],-1)\n          input_s = paste(input_s, collapse = \" \")\n          u1 = paste(input_s, u1)\n          updateTextInput(session, \"Sentence\", value = u1)\n        })\n        \n        observeEvent(input$Keep, {\n          load(\"Sentence.RData\")\n          #test_word = \"Leaving EU would liberate UK\"\n          sentence = strsplit(test_word, \" \")\n          sentence = head(sentence[[1]])\n          sentence = paste(sentence, collapse = \" \")\n          pred_output1 = paste(\"Prediction 1:\", sentence, \"|\", as.character(unknowns[1,1]))\n          pred_output2 = paste(\"Prediction 2:\", sentence, \"|\", as.character(unknowns[1,2]))\n          pred_output3 = paste(\"Prediction 3:\", sentence, \"|\", as.character(unknowns[1,3]))\n          save(pred_output1, file = \"pred_output1.RData\")\n          save(pred_output2, file = \"pred_output2.RData\")\n          save(pred_output3, file = \"pred_output3.RData\")\n          output$text1 <- renderText({{pred_output1}})\n          output$text2 <- renderText({{pred_output2}})\n          output$text3 <- renderText({{pred_output3}})\n        })\n        \n    }\n)",
    "created" : 1459258276065.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3667479633",
    "id" : "88132D86",
    "lastKnownWriteTime" : 1461055933,
    "path" : "C:/Users/rob.harrand/Desktop/WORK/Coursera/Module 10 - Capstone/Shiny/server.R",
    "project_path" : "Shiny/server.R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}