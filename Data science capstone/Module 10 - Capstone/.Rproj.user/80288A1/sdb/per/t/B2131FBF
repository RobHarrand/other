{
    "contents" : "options( java.parameters = \"-Xmx4g\" ) #Set memory options\n\n#Load libraries,\nlibrary(\"tm\")\nlibrary(\"SnowballC\")\nlibrary(\"stringr\")\nlibrary(\"readr\")\nlibrary('stringdist')\nlibrary(RWeka)\n\n#Load text files,\nen_blogs = read_lines(\"texts/en_US/en_US.blogs.txt\")\nen_tweets = readLines(\"texts/en_US/en_US.twitter.txt\", skipNul = T)\nen_news = read_lines(\"texts/en_US/en_US.news.txt\")\ndict = read_lines(\"texts/Dict.txt\")\n\nprop = 0.02\n\nset.seed(100)\nen_blogs_sample = sample(en_blogs, (length(en_blogs)*prop))\nset.seed(100)\nen_tweets_sample = sample(en_tweets, (length(en_tweets)*prop))\nset.seed(100)\nen_news_sample = sample(en_news, (length(en_news)*prop))\n\nen_blogs_sample = iconv(en_blogs_sample, \"latin1\", \"ASCII\", \"\")\nen_tweets_sample = iconv(en_tweets_sample, \"latin1\", \"ASCII\", \"\")\nen_news_sample = iconv(en_news_sample, \"latin1\", \"ASCII\", \"\")\n\nrm(en_blogs)\nrm(en_tweets)\nrm(en_news)\n\nall_sample = c(en_blogs_sample, en_tweets_sample, en_news_sample)\nall_sample_corpus = Corpus(VectorSource(list(all_sample)))\n\nrm(en_blogs_sample)\nrm(en_tweets_sample)\nrm(en_news_sample)\nrm(all_sample)\n\n\n#Tidy\n#---------------------------------------\n\n#Remove punctuation â€“ replace punctuation marks with a space\nall_sample_corpus = tm_map(all_sample_corpus, removePunctuation)\nall_sample_corpus = tm_map(all_sample_corpus, content_transformer(tolower))\nall_sample_corpus = tm_map(all_sample_corpus, removeNumbers) \nall_sample_corpus = tm_map(all_sample_corpus, stripWhitespace)\n\n\n#Remove swearwords,\n\nswearwords = read.delim(\"swearWords.txt\", sep = \"\\n\", header = F)\nall_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)\nrm(swearwords)\n\n\n#Save the corpus\n#-------------------------------------\n\n#writeCorpus(all_sample_corpus, filenames=\"all_sample_corpus.txt\")\n#all_10pc <- readLines(\"all_sample_corpus.txt\")\n#all_sample_corpus = Corpus(VectorSource(list(all_10pc)))\n\n\n#Wordcloud\n#-------------------------------------------------\n\n#library(wordcloud)\n#set.seed(100)\n#wordcloud(names(freq),freq, min.freq=500,colors=brewer.pal(6,\"Dark2\"))\n\n\n#Ngrams (RWeka package)\n#-------------------------------------------------\n\nOnegramTokenizer = function(x) WordTokenizer(x)\ndtm = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = OnegramTokenizer, wordLengths = c(1, Inf)))\n\nfreq = colSums(as.matrix(dtm)) \ncounts_dtm = data.frame(Word=names(freq), Freq=freq) \ncounts_dtm = counts_dtm[counts_dtm$Freq>2,] #Pruning\n\ncounts_dtm$Word = droplevels(counts_dtm$Word)\nsave(counts_dtm, file = \"counts_dtm.RData\")\nsave(freq, file = \"freq.RData\")\nrm(dtm)\nrm(counts_dtm)\nrm(freq)\n\n\nTwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\ndtm2 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = TwogramTokenizer))\n\nfreq2 = colSums(as.matrix(dtm2)) \ncounts_dtm2 = data.frame(Word=names(freq2), Freq=freq2) \ncounts_dtm2 = counts_dtm2[counts_dtm2$Freq>2,] #Pruning\n\ncounts_dtm2$Word = droplevels(counts_dtm2$Word)\nsave(counts_dtm2, file = \"counts_dtm2.RData\")\nsave(freq2, file = \"freq2.RData\")\nrm(dtm2)\nrm(counts_dtm2)\nrm(freq2)\n\n\nThreegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\ndtm3 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = ThreegramTokenizer))\n\nfreq3 = colSums(as.matrix(dtm3))\ncounts_dtm3 = data.frame(Word=names(freq3), Freq=freq3) \ncounts_dtm3 = counts_dtm3[counts_dtm3$Freq>2,] #Pruning\n\ncounts_dtm3$Word = droplevels(counts_dtm3$Word)\nsave(counts_dtm3, file = \"counts_dtm3.RData\")\nsave(freq3, file = \"freq3.RData\")\nrm(dtm3)\nrm(counts_dtm3)\nrm(freq3)\n\n\nFourgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))\ndtm4 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FourgramTokenizer))\n\nfreq4 = colSums(as.matrix(dtm4)) \ncounts_dtm4 = data.frame(Word=names(freq4), Freq=freq4) \ncounts_dtm4 = counts_dtm4[counts_dtm4$Freq>2,] #Pruning\n\ncounts_dtm4$Word = droplevels(counts_dtm4$Word)\nsave(counts_dtm4, file = \"counts_dtm4.RData\")\nsave(freq4, file = \"freq4.RData\")\nrm(dtm4)\nrm(counts_dtm4)\nrm(freq4)\n\n\nFivegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))\ndtm5 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FivegramTokenizer))\n\nfreq5 = colSums(as.matrix(dtm5)) \ncounts_dtm5 = data.frame(Word=names(freq5), Freq=freq5) \ncounts_dtm5 = counts_dtm5[counts_dtm5$Freq>2,] #Pruning\n\ncounts_dtm5$Word = droplevels(counts_dtm5$Word)\nsave(counts_dtm5, file = \"counts_dtm5.RData\")\nsave(freq5, file = \"freq5.RData\")\nrm(dtm5)\nrm(counts_dtm5)\nrm(freq5)\n\n\n#Save and load\n#------------------------------------\n\n# save(dtm, file = \"dtm.RData\")\n# save(dtm2, file = \"dtm2.RData\")\n# save(dtm3, file = \"dtm3.RData\")\n# save(dtm4, file = \"dtm4.RData\")\n# save(dtm5, file = \"dtm5.RData\")\n# \n# save(dtm_test, file = \"dtm_test.RData\")\n# save(dtm2_test, file = \"dtm2_test.RData\")\n# save(dtm3_test, file = \"dtm3_test.RData\")\n# save(dtm4_test, file = \"dtm4_test.RData\")\n# save(dtm5_test, file = \"dtm5_test.RData\")\n# \nsave(all_sample_corpus, file = \"all_sample_corpus.RData\")\n#save(all_sample_corpus_test, file = \"all_sample_corpus_test\")\n\n\n# load(dtm_test, file = \"dtm_test.RData\")\n# load(dtm2_test, file = \"dtm2_test.RData\")\n# load(dtm3_test, file = \"dtm3_test.RData\")\n# load(dtm4_test, file = \"dtm4_test.RData\")\n# load(dtm5_test, file = \"dtm5_test.RData\")\n\n#load(all_sample_corpus_test, file = \"all_sample_corpus_test\")\n\n\nload(\"counts_dtm.RData\")\nload(\"counts_dtm2.RData\")\nload(\"counts_dtm3.RData\")\nload(\"counts_dtm4.RData\")\nload(\"counts_dtm5.RData\")\n\n#Maximum likelihood estimate for a bigram, P(Wi | Wi-1) = count(Wi-1, Wi) / count(Wi-1)\n#For example, P(Park | Car) = the number of bigrams of 'Car Park' divided by the number of unigrams of 'Car'\n\n#Unigram probabilities\nUni_length = sum(counts_dtm$Freq)\ncounts_dtm$Prob = counts_dtm$Freq / Uni_length\n\n\n\n#Bigram probabiltiies\ncounts_dtm2$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm2$Word)) {\n  \n  uni_count = counts_dtm$Freq[counts_dtm$Word == (strsplit(as.character(counts_dtm2$Word[i]), \" \")[[1]][1])]\n  bi_count = counts_dtm2$Freq[i]\n  counts_dtm2$Prob[i] = bi_count / uni_count\n  i=i+1\n  print((i/length(counts_dtm2$Word)*100))\n  \n}\n\n\n\n#Trigram probabiltiies\n\n#Maximum likelihood estimate for a trigram, P(wi | wi-1 wi-2 ) = count(wi, wi-1, wi-2 ) / count(wi-1, wi-2 )\n#For example, P(Park | The car) = the number of trigrams of 'The car park' divided by the number of bigrams of 'The car'\n\ncounts_dtm3$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm3$Word)) {\n  \n  bi_count = counts_dtm2$Freq[counts_dtm2$Word == paste(strsplit(as.character(counts_dtm3$Word[i]), \" \")[[1]][1],\n                                                        strsplit(as.character(counts_dtm3$Word[i]), \" \")[[1]][2])]\n  tri_count = counts_dtm3$Freq[i]\n  counts_dtm3$Prob[i] = tri_count / bi_count\n  i=i+1\n  print((i/length(counts_dtm3$Word)*100))\n  \n}\n\n\n\n#Quadgram probabiltiies\n\ncounts_dtm4$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm4$Word)) {\n  \n  tri_count = counts_dtm3$Freq[counts_dtm3$Word == paste(strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][1],\n                                                         strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][2],\n                                                         strsplit(as.character(counts_dtm4$Word[i]), \" \")[[1]][3])]\n  quad_count = counts_dtm4$Freq[i]\n  counts_dtm4$Prob[i] = quad_count / tri_count\n  i=i+1\n  print((i/length(counts_dtm4$Word)*100))\n  \n}\n\n\n#Fivegram probabiltiies\n\ncounts_dtm5$Prob = 0\ni=1\n\nwhile (i <= length(counts_dtm5$Word)) {\n  \n  quad_count = counts_dtm4$Freq[counts_dtm4$Word == paste(strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][1],\n                                                          strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][2],\n                                                          strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][3],\n                                                          strsplit(as.character(counts_dtm5$Word[i]), \" \")[[1]][4])]\n  five_count = counts_dtm5$Freq[i]\n  counts_dtm5$Prob[i] = five_count / quad_count\n  i=i+1\n  print((i/length(counts_dtm5$Word)*100))\n  \n}\n\n\n\n#Simple interpolation (lambdas are static for all cases)\n#------------------------------------------------------------------\n\n\nsave(counts_dtm, file = \"counts_dtm.RData\")\nsave(counts_dtm2, file = \"counts_dtm2.RData\")\nsave(counts_dtm3, file = \"counts_dtm3.RData\")\nsave(counts_dtm4, file = \"counts_dtm4.RData\")\nsave(counts_dtm5, file = \"counts_dtm5.RData\")\n\n\n\n\n\n#Kneser-Ney smoothing\n#---------------------------------------------\n\n#First, we need the corpus as bigrams (not just the unique bigrams, but the whole lot),\n\nbis = as.data.frame(strsplit(as.character(counts_dtm2$Word), \" \"))\nbis = as.data.frame(t(bis))\nbis = cbind(bis, counts_dtm2$Freq)\ncolnames(bis) = c(\"Word1\", \"Word2\", \"Freq\")\n\n\n#Next, we need to know what are the most common 2nd words in the bigrams\n\nmost_bi = sort(bis$Freq, decreasing = T)\nmost_bis = bis[bis$Freq %in% most_bi,]\n\nmost_bis = most_bis[order(-most_bis$Freq),]\n\nmost_bis = droplevels(most_bis)\n\nunique_w2 = unique(most_bis$Word2)\nunique_w1 = unique(most_bis$Word1)\n\nlength_element_w1 = length(unique_w1)\n\n#Then, from these common endings, we need to know how many bigrams each completes\n#Finally, we work out the continuum probability based upon this\n\nkn = data.frame()\ni=1\n\nwhile (i <= length(unique_w2)) {\n  \n  sum_element = sum(most_bis$Freq[most_bis$Word2 == unique_w2[i]])\n  length_element_w2 = length(most_bis$Word1[most_bis$Word2 == unique_w2[i]])\n  prob_element = length_element_w2 / length_element_w1 #No. of word types seen to precede / no. of words preceding all words\n  kn[i,1] = unique_w2[i]\n  kn[i,2] = sum_element\n  kn[i,3] = length_element_w2\n  kn[i,4] = prob_element\n  i=i+1\n  print(i/length(unique_w2)*100)\n  \n}\n\ncolnames(kn) = c(\"2nd Word\", \"Freq\", \"No. of bigrams it completes\", \"Pcont\")\n\nkn = kn[order(-kn$Pcont),]\n#save(kn, file = \"kn.RData\")\n\n\n\n#Things to load\n#-------------------------------------------\n\ndict = read_lines(\"texts/Dict.txt\")\n\nload(file = \"counts_dtm_2pc.RData\")\nload(file = \"counts_dtm2_2pc.RData\")\nload(file = \"counts_dtm3_2pc.RData\")\nload(file = \"counts_dtm4_2pc.RData\")\nload(file = \"counts_dtm5_2pc.RData\")\n\nload(file = \"kn_2pc.RData\")\n\nunknown1 = as.character(kn[1,1])\nunknown2 = as.character(kn[2,1])\nunknown3 = as.character(kn[3,1])\n\nunknowns = data.frame(unknown1,unknown2,unknown3)\n\n\n\n\n#Prediction function\n#-------------------------------------\n\nNWP = function(new_word, unknowns) {\n  \n  new_word_full = new_word\n  \n  new_word_full = tolower(new_word_full)\n  new_word_full = removePunctuation(new_word_full)\n  new_word_full = removeNumbers(new_word_full)\n  new_word_full = stripWhitespace(new_word_full)\n  new_word_full = str_trim(new_word_full)\n  \n  new_word_full_original = new_word_full\n  \n  new_word_split = strsplit(new_word_full, \" \")\n  \n  len_new_word = length(new_word_split[[1]])\n  \n  i=1\n  \n  \n  if (length(new_word_split[[1]]) > 4) {\n    \n    new_word_full = paste(new_word_split[[1]][len_new_word-3],new_word_split[[1]][len_new_word-2],\n                          new_word_split[[1]][len_new_word-1], new_word_split[[1]][len_new_word])\n    \n    new_word_split = strsplit(new_word_full, \" \")\n    \n  } \n  \n  no_match = 0\n  \n\n  #5GRAM CHECKER\n  ---------------------\n    \n    if(length(new_word_split[[1]]) <= 4) {\n      \n      if(length(new_word_split[[1]]) == 4) {\n        \n        Five_matches = as.character(counts_dtm5$Word[grep(paste(\"^\", new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        Five_matches_f = as.numeric(counts_dtm5$Freq[grep(paste(\"^\", new_word_full, \"\\\\b\", sep = \"\"), counts_dtm5$Word)])\n        \n        if (length(Five_matches) == 0) {\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3],new_word_split[[1]][4])\n          new_word_split = strsplit(new_word_full, \" \")\n\n        } \n        \n        else {\n          \n          Five_split = strsplit(Five_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Five_split)) {\n            \n            Token[i] = as.list(paste(Five_split[[1]][1], Five_split[[1]][2], Five_split[[1]][3], Five_split[[1]][4]))\n            Token[[i]][2] = Five_split[[i]][5]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Five_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Five_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Five_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred5 = data.frame()\n          \n          while (i <= x) {\n            \n            pred5[i,1] = (paste(\"(Fivegram)\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred5[i,2] = markov[i,2]\n            pred5[i,3] = counts_dtm5$Prob[counts_dtm5$Word == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred5) = c(\"Fivegram Prediction\", \"Freq\", \"Prob\")\n          pred5$Prob = round(pred5$Prob, 3)\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3],new_word_split[[1]][4])\n          new_word_split = strsplit(new_word_full, \" \")\n\n        }\n      }\n      \n      \n        \n      if(length(new_word_split[[1]]) == 3) { #QUADGRAM CHECKER   \n      \n        Quad_matches = as.character(counts_dtm4$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        Quad_matches_f = as.numeric(counts_dtm4$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm4$Word)])\n        \n        if (length(Quad_matches) == 0 | (sum(Quad_matches_f) / length(Quad_matches_f)) < 2) {\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3])\n          new_word_split = strsplit(new_word_full, \" \")\n\n        } \n        \n        else {\n          \n          Quad_split = strsplit(Quad_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Quad_split)) {\n            \n            Token[i] = as.list(paste(Quad_split[[1]][1], Quad_split[[1]][2], Quad_split[[1]][3]))\n            Token[[i]][2] = Quad_split[[i]][4]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Quad_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Quad_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Quad_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred4 = data.frame()\n          \n          while (i <= x) {\n            \n            pred4[i,1] = (paste(\"(Quadgram)\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred4[i,2] = markov[i,2]\n            pred4[i,3] = counts_dtm4$Prob[counts_dtm4$Word == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred4) = c(\"Quadgram Prediction\", \"Freq\", \"Prob\")\n          pred4$Prob = round(pred4$Prob, 3)\n          \n          new_word_full = paste(new_word_split[[1]][2],new_word_split[[1]][3])\n          new_word_split = strsplit(new_word_full, \" \")\n        }\n      \n      }\n      \n        \n        \n      if(length(new_word_split[[1]]) == 2) { #TRIGRAM CHECKER  \n      \n        Tri_matches = as.character(counts_dtm3$Word[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        Tri_matches_f = as.numeric(counts_dtm3$Freq[grep(paste(\"^\",new_word_full, \"\\\\b\", sep = \"\"), counts_dtm3$Word)])\n        \n        \n        if (length(Tri_matches) == 0 | (sum(Tri_matches_f) / length(Tri_matches_f)) < 2) {\n          \n          new_word_full = new_word_split[[1]][2]\n          new_word_split = strsplit(new_word_full, \" \")\n\n        } \n        \n        else {\n          \n          Tri_split = strsplit(Tri_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Tri_split)) {\n            \n            Token[i] = as.list(paste(Tri_split[[1]][1], Tri_split[[1]][2]))\n            Token[[i]][2] = Tri_split[[i]][3]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Tri_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Tri_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Tri_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred3 = data.frame()\n          \n          while (i <= x) {\n            \n            pred3[i,1] = (paste(\"(Trigram)\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred3[i,2] = markov[i,2]\n            pred3[i,3] = counts_dtm3$Prob[counts_dtm3 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred3) = c(\"Trigram Prediction\", \"Freq\", \"Prob\")\n          pred3$Prob = round(pred3$Prob, 3)\n          \n          new_word_full = new_word_split[[1]][2]\n          new_word_split = strsplit(new_word_full, \" \")\n          \n        }\n      }\n      \n      \n      if(length(new_word_split[[1]]) == 1) { #BIGRAM CHECKER      \n\n        Bi_matches = as.character(counts_dtm2$Word[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        Bi_matches_f = as.numeric(counts_dtm2$Freq[grep(paste(\"^\\\\b\",new_word_full, \"\\\\b\",\"\\\\b\", sep = \"\"), counts_dtm2$Word)])\n        \n        if (length(Bi_matches) == 0) {\n          \n          no_match = 1\n\n        } \n        \n        else {\n          \n          Bi_split = strsplit(Bi_matches, \" \")\n          \n          i=1\n          Token = \"\"\n          \n          while (i <= length(Bi_split)) {\n            \n            Token[i] = as.list(Bi_split[[1]][1])\n            Token[[i]][2] = Bi_split[[i]][2]\n            i=i+1\n          }\n          \n          markov = data.frame(1:length(Bi_split))\n          markov$Word = \" \"\n          markov[,1] = NULL\n          \n          \n          i = 1\n          \n          while (i <= length(Bi_split)) {\n            \n            markov$Word[i] = Token[[i]][2]\n            i=i+1\n            \n          }\n          \n          markov$No = as.numeric(Bi_matches_f)\n          markov_total = sum(markov$No)\n          markov$Prob = markov$No/markov_total\n          colnames(markov) = c(\"Word\", \"No\", \"Prob\")\n          \n          markov = markov[order(-markov$Prob),]\n          \n          i=1\n          x = length(markov$Word)\n          if (x > 10) {x = 10}\n          \n          pred2 = data.frame()\n          \n          while (i <= x) {\n            \n            pred2[i,1] = (paste(\"(Bigram)\", new_word_full_original, \"|\", as.character(markov$Word[i])))\n            pred2[i,2] = markov[i,2]\n            pred2[i,3] = counts_dtm2$Prob[counts_dtm2 == (paste(new_word_full, as.character(markov$Word[i])))]\n            i=i+1\n            \n          }\n          \n          colnames(pred2) = c(\"Bigram Prediction\", \"Freq\", \"Prob\")\n          pred2$Prob = round(pred2$Prob, 3)\n          no_match = 1\n          \n        }\n      }\n        \n      \n      \n      \n      \n      #No match from a single word\n      #---------------------------------\n      \n      if (no_match == 1) {\n        \n        if (new_word_split %in% dict) {\n\n          pred1 = data.frame()\n          pred1[1,1] = (paste(\"(Unigram)\", new_word_full_original, \"|\", as.character(unknowns[1,1])))\n          pred1[2,1] = (paste(\"(Unigram)\", new_word_full_original, \"|\", as.character(unknowns[1,2])))\n          pred1[3,1] = (paste(\"(Unigram)\", new_word_full_original, \"|\", as.character(unknowns[1,3])))\n          pred1[1,2] = 1\n          pred1[1,3] = 0.001\n          pred1[2,2] = 1\n          pred1[2,3] = 0.001\n          pred1[3,2] = 1\n          pred1[3,3] = 0.001\n          colnames(pred1) = c(\"Unigram Prediction\", \"Freq\", \"Prob\")\n\n        } else {\n          \n          nearest = amatch(new_word_split, counts_dtm$Word[counts_dtm$Word != new_word_split], maxDist = 5, method = \"lcs\")\n          \n          pred1 = data.frame()\n          pred1[1,1] = paste(\"Word not found. Did you mean '\", counts_dtm$Word[nearest], \"' ?\", sep = \"\")\n          pred1[1,2] = \"2\"\n          pred1[1,3] = \"2\"\n          \n        }\n        \n      } \n      \n      if (!exists(\"pred5\")) {pred5 = data.frame(\"No Fivegram match\", \" \", \" \")}\n      if (!exists(\"pred4\")) {pred4 = data.frame(\"No Quadgram match\", \" \", \" \")}\n      if (!exists(\"pred3\")) {pred3 = data.frame(\"No Trigram match\", \" \", \" \")}\n      if (!exists(\"pred2\")) {pred2 = data.frame(\"No Bigram match\", \" \", \" \")}\n      if (!exists(\"pred1\")) {pred1 = data.frame(\"No Unigram match\", \" \", \" \")}\n        \n      colnames(pred5) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred4) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred3) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred2) = c(\"Prediction\", \"Freq\", \"Prob\")\n      colnames(pred1) = c(\"Prediction\", \"Freq\", \"Prob\")\n      \n      pred5[,1] = as.character(pred5[,1])\n      pred5$Freq = as.numeric(pred5$Freq)\n      pred5$Prob = as.numeric(pred5$Prob)\n      \n      pred4[,1] = as.character(pred4[,1])\n      pred4$Freq = as.numeric(pred4$Freq)\n      pred4$Prob = as.numeric(pred4$Prob)\n      \n      pred3[,1] = as.character(pred3[,1])\n      pred3$Freq = as.numeric(pred3$Freq)\n      pred3$Prob = as.numeric(pred3$Prob)\n      \n      pred2[,1] = as.character(pred2[,1])\n      pred2$Freq = as.numeric(pred2$Freq)\n      pred2$Prob = as.numeric(pred2$Prob)\n      \n      pred1[,1] = as.character(pred1[,1])\n      pred1$Freq = as.numeric(pred1$Freq)\n      pred1$Prob = as.numeric(pred1$Prob)\n      \n      pred_all = rbind(pred5, pred4, pred3, pred2, pred1)\n        \n      return(pred_all)\n      \n    }\n}\n\n\n\n#Adjust the probabilities \n#------------------------------------\n\nl1 = 0.03125\nl2 = 0.0625\nl3 = 0.125\nl4 = 0.25\nl5 = 0.53125\n\n\n#Adjust Fivegrams\n#----------------\n\ni=1\n\nif (prediction5$Prediction[1] != 'No Fivegram match') {\n\n  while (i <= length(prediction5$Prediction)) {\n  \n  split = strsplit(prediction4$Prediction[i], \" \")\n  \n  prediction5$Adjust_prob[i] =  round(l5*(prediction5$Prob[i]) + \n                                  l4*(counts_dtm4$Prob[counts_dtm4$Word == paste(split[[1]][length(split[[1]])-4],split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                  l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                  l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                  l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n  \n  i=i+1\n  }\n  \n} else {prediction5$Adjust_prob[1] = 0}\n\n\n#Adjust Fourgrams\n#----------------\n\ni=1\n\nif (prediction4$Prediction[1] != 'No Quadgram match') {\n  \n  while (i <= length(prediction4$Prediction)) {\n    \n    split = strsplit(prediction4$Prediction[i], \" \")\n    \n    prediction4$Adjust_prob[i] =  round(l4*(prediction4$Prob[i]) +\n                                    l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                    l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                    l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n    \n    i=i+1\n  }\n  \n} else {prediction4$Adjust_prob[1] = 0}\n\n\n\n\n#Adjust Trigrams\n#----------------\n\ni=1\n\nif (prediction3$Prediction[1] != 'No Trigram match') {\n\n  while (i <= length(prediction3$Prediction)) {\n    \n    split = strsplit(prediction3$Prediction[i], \" \")\n    \n    prediction3$Adjust_prob[i] =  round(l3*(prediction3$Prob[i]) + \n                                       l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                       l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n    \n    i=i+1\n  }\n}  else {prediction3$Adjust_prob[1] = 0}\n  \n\n#Adjust Bigrams\n#----------------\n\ni=1\n\nif (prediction2$Prediction[1] != 'No Bigram match') {\n\n  while (i <= length(prediction2$Prediction)) {\n    \n    split = strsplit(prediction2$Prediction[i], \" \")\n    \n    prediction2$Adjust_prob[i] =  round(l2*(prediction2$Prob[i]) + \n                                    l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n    \n    i=i+1\n  } \n} else {prediction2$Adjust_prob[1] = 0}\n\n\n#Adjust Unigrams\n#----------------\n\ni=1\n\n  while (i <= length(prediction1$Prediction)) {\n    \n    prediction1$Adjust_prob[i] =  l1*(prediction1$Prob[i])\n    \n    i=i+1\n}\n\n\n\n\n#Re-merge,\n\npred_all = rbind(prediction5, \n                 prediction4, \n                 prediction3, \n                 prediction2, \n                 prediction1)\n\n\npred_all = pred_all[order(-pred_all$Adjust_prob),]\n\npred_all\n\n\n\n#Wordcloud\n#-------------------------------------------------\n\nlibrary(wordcloud)\nset.seed(100)\n\npred_word = pred_all[(!grepl(\"No Fivegram|No Quadgram|No Trigram|No Bigram|Word not found\", pred_all$Prediction)),]\npred_words = strsplit(pred_word$Prediction, \" | \")\npred_words = pred_words\n\ni=1\npred_wordcloud = data.frame()\n\nwhile (i <= length(pred_words)) {\n\n  pred_wordcloud[i,1] = pred_words[[i]][length(pred_words[[i]])]\n  pred_wordcloud[i,2] = pred_words[[i]][length(pred_words[[i]])]\n  i=i+1\n  \n}\n\npred_wordcloud$Freq = pred_word$Adjust_prob*1000\ncolnames(pred_wordcloud) = c(\"Word1\", \"Word2\", \"Freq\")\n\nif (nrow(pred_word) >3) {\n\n  wordcloud(pred_wordcloud$Word1,pred_wordcloud$Freq, min.freq=1,colors=brewer.pal(6,\"Dark2\"))\n\n}\n  \n\n\n#Evaluation\n#-----------------------------------------\n\n\n#Create test set\n\nen_blogs = read_lines(\"texts/en_US/en_US.blogs.txt\")\nen_tweets = readLines(\"texts/en_US/en_US.twitter.txt\", skipNul = T)\nen_news = read_lines(\"texts/en_US/en_US.news.txt\")\n\nprop = 0.01\n\nset.seed(500)\nen_blogs_sample_test = sample(en_blogs, (length(en_blogs)*prop))\nset.seed(500)\nen_tweets_sample_test = sample(en_tweets, (length(en_tweets)*prop))\nset.seed(500)\nen_news_sample_test = sample(en_news, (length(en_news)*prop))\n\nen_blogs_sample_test = iconv(en_blogs_sample_test, \"latin1\", \"ASCII\", \"\")\nen_tweets_sample_test = iconv(en_tweets_sample_test, \"latin1\", \"ASCII\", \"\")\nen_news_sample_test = iconv(en_news_sample_test, \"latin1\", \"ASCII\", \"\")\n\nall_sample_test = c(en_blogs_sample_test, en_tweets_sample_test, en_news_sample_test)\nall_sample_corpus_test = Corpus(VectorSource(list(all_sample_test)))\n\nrm(en_blogs_sample_test)\nrm(en_tweets_sample_test)\nrm(en_news_sample_test)\nrm(all_sample_test)\n\nOnegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))\ndtm_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = OnegramTokenizer))\n\nTwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\ndtm2_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = TwogramTokenizer))\n\nThreegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\ndtm3_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = ThreegramTokenizer))\n\nFourgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))\ndtm4_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = FourgramTokenizer))\n\nFivegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))\ndtm5_test <- DocumentTermMatrix(all_sample_corpus_test, control = list(tokenize = FivegramTokenizer))\n\nfreq_test = colSums(as.matrix(dtm_test)) \ncounts_dtm_test = data.frame(Word=names(freq_test), Freq=freq_test) \ncounts_dtm_test = counts_dtm_test[counts_dtm_test$Freq>2,]\n\nfreq2_test = colSums(as.matrix(dtm2_test)) \ncounts_dtm2_test = data.frame(Word=names(freq2_test), Freq=freq2_test) \ncounts_dtm2_test = counts_dtm2_test[counts_dtm2_test$Freq>2,]\n\nfreq3_test = colSums(as.matrix(dtm3_test))\ncounts_dtm3_test = data.frame(Word=names(freq3_test), Freq=freq3_test) \ncounts_dtm3_test = counts_dtm3_test[counts_dtm3_test$Freq>2,]\n\nfreq4_test = colSums(as.matrix(dtm4_test)) \ncounts_dtm4_test = data.frame(Word=names(freq4_test), Freq=freq4_test) \ncounts_dtm4_test = counts_dtm4_test[counts_dtm4_test$Freq>2,]\n\nfreq5_test = colSums(as.matrix(dtm5_test)) \ncounts_dtm5_test = data.frame(Word=names(freq5_test), Freq=freq5_test) \ncounts_dtm5_test = counts_dtm5_test[counts_dtm5_test$Freq>2,]\n\n\ntest_set_2 = as.character(sample(counts_dtm2_test$Word, size = 25))\ntest_set_3 = as.character(sample(counts_dtm3_test$Word, size = 25))\ntest_set_4 = as.character(sample(counts_dtm4_test$Word, size = 25))\ntest_set_5 = as.character(sample(counts_dtm5_test$Word, size = 25))\n\ntest_set = c(test_set_3,test_set_4,test_set_5)\n\n\n\ny = 1\ncounter = 0\neval = data.frame()\n\nwhile (y <= length(test_set)) {\n  \n  test = test_set[y]\n  test_split = strsplit(test, \" \")\n  len_new_word = length(test_split[[1]])\n  \n  \n  if (length(test_split[[1]]) > 4) {\n    \n    test = paste(test_split[[1]][len_new_word-3],test_split[[1]][len_new_word-2],\n                 test_split[[1]][len_new_word-1], test_split[[1]][len_new_word])\n    \n    test_split = strsplit(test, \" \")\n    \n  } \n  \n  \n  test_split_partial = test_split[[1]][1:length(test_split[[1]])-1]\n  \n  if (length(test_split_partial) == 4) {test_func=paste(test_split_partial[1],test_split_partial[2],test_split_partial[3],test_split_partial[4])}\n  if (length(test_split_partial) == 3) {test_func=paste(test_split_partial[1],test_split_partial[2],test_split_partial[3])}\n  if (length(test_split_partial) == 2) {test_func=paste(test_split_partial[1],test_split_partial[2])}\n  if (length(test_split_partial) == 1) {test_func=test_split_partial[1]}\n  \n  p_return = NWP(test_func, unknowns)\n  \n  prediction5 = p_return[grep(\"Five\", p_return$Prediction),]\n  prediction4 = p_return[grep(\"Quad\", p_return$Prediction),]\n  prediction3 = p_return[grep(\"Tri\", p_return$Prediction),]\n  prediction2 = p_return[grep(\"Bi\", p_return$Prediction),]\n  \n  if (length(grep(\"Uni\", p_return$Prediction)) != 0) {\n    \n    prediction1 = p_return[grep(\"Uni\", p_return$Prediction),]\n    \n  } else {prediction1 = p_return[grep(\"not found\", p_return$Prediction),]}\n  \n  \n  \n  #Adjust the probabilities \n  #------------------------------------\n  \n  l1 = 0.03125\n  l2 = 0.0625\n  l3 = 0.125\n  l4 = 0.25\n  l5 = 0.53125\n\n  \n  #Adjust Fivegrams\n  #----------------\n  \n  i=1\n  \n  if (prediction5$Prediction[1] != 'No Fivegram match') {\n    \n    while (i <= length(prediction5$Prediction)) {\n      \n      split = strsplit(prediction4$Prediction[i], \" \")\n      \n      prediction5$Adjust_prob[i] =  round(l5*(prediction5$Prob[i]) + \n                                            l4*(counts_dtm4$Prob[counts_dtm4$Word == paste(split[[1]][length(split[[1]])-4],split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                            l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                            l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                            l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n      \n      i=i+1\n    }\n    \n  } else {prediction5$Adjust_prob[1] = 0}\n  \n  \n  #Adjust Fourgrams\n  #----------------\n  \n  i=1\n  \n  if (prediction4$Prediction[1] != 'No Quadgram match') {\n    \n    while (i <= length(prediction4$Prediction)) {\n      \n      split = strsplit(prediction4$Prediction[i], \" \")\n      \n      prediction4$Adjust_prob[i] =  round(l4*(prediction4$Prob[i]) +\n                                            l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                            l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                            l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n      \n      i=i+1\n    }\n    \n  } else {prediction4$Adjust_prob[1] = 0}\n  \n  \n  \n  \n  #Adjust Trigrams\n  #----------------\n  \n  i=1\n  \n  if (prediction3$Prediction[1] != 'No Trigram match') {\n    \n    while (i <= length(prediction3$Prediction)) {\n      \n      split = strsplit(prediction3$Prediction[i], \" \")\n      \n      prediction3$Adjust_prob[i] =  round(l3*(prediction3$Prob[i]) + \n                                            l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                            l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n      \n      i=i+1\n    }\n  }  else {prediction3$Adjust_prob[1] = 0}\n  \n  \n  #Adjust Bigrams\n  #----------------\n  \n  i=1\n  \n  if (prediction2$Prediction[1] != 'No Bigram match') {\n    \n    while (i <= length(prediction2$Prediction)) {\n      \n      split = strsplit(prediction2$Prediction[i], \" \")\n      \n      prediction2$Adjust_prob[i] =  round(l2*(prediction2$Prob[i]) + \n                                            l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n      \n      i=i+1\n    } \n  } else {prediction2$Adjust_prob[1] = 0}\n  \n  \n  #Adjust Unigrams\n  #----------------\n  \n  i=1\n  \n  while (i <= length(prediction1$Prediction)) {\n    \n    prediction1$Adjust_prob[i] =  l1*(prediction1$Prob[i])\n    \n    i=i+1\n  }\n  \n  \n  \n  \n  #Re-merge,\n  \n  pred_all = rbind(prediction5, \n                   prediction4, \n                   prediction3, \n                   prediction2, \n                   prediction1)\n  \n  \n  pred_all = pred_all[order(-pred_all$Adjust_prob),]\n  pred_all\n  \n  pred_all_split = strsplit(pred_all$Prediction, \" | \")\n\n  \n  \n  \n#Make the top 3 results unique,\n  \nif (pred_all_split[[1]][length(pred_all_split[[1]])] == pred_all_split[[2]][length(pred_all_split[[2]])]) {\n  \n  pred_all = pred_all[-2,]\n  \n}\n  \n  pred_all_split = strsplit(pred_all$Prediction, \" | \")\n  \n  if (pred_all_split[[2]][length(pred_all_split[[2]])] == pred_all_split[[3]][length(pred_all_split[[3]])]) {\n    \n    pred_all = pred_all[-3,]\n    \n  }\n  \n  \n  \n  p1 = pred_all[1,1]\n  p1 = strsplit(p1, \" | \")\n  p1 = p1[[1]][length(p1[[1]])]\n  \n  if (dim(pred_all)[1] >= 2) {\n    \n    p2 = pred_all[2,1]\n    p2 = strsplit(p2, \" | \")\n    p2 = p2[[1]][length(p2[[1]])]\n    \n  } else {p2 = \" \"}\n  \n  if (dim(p_return)[1] >= 3) {\n    \n    p3 = pred_all[3,1]\n    p3 = strsplit(p3, \" | \")\n    p3 = p3[[1]][length(p3[[1]])]\n    \n  } else {p3 = \" \"}\n  \n  \n  eval[y,1] = test\n  eval[y,2] = p1\n  eval[y,3] = p2\n  eval[y,4] = p3\n  \n  if (test_split[[1]][length(test_split[[1]])] == eval[y,2] | \n      test_split[[1]][length(test_split[[1]])] == eval[y,3] | \n      test_split[[1]][length(test_split[[1]])] == eval[y,4]) {counter = counter+1}\n  \n  print(y/length(test_set)*100)\n  y=y+1\n  \n}\n\n\naccuracy = counter / length(test_set)\naccuracy*100\n\neval\n\n\n\n\n\n#Lambda generator\n#----------------------------------\n\nlambda_results = data.frame()\ni=1\n\nwhile (i <= 50) {\n  \n  order = sample(1:5)\n  \n  l1 = runif(1)\n  l2 = runif(1, min = 0, max = (1-l1))\n  l3 = runif(1, min = 0, max = (1-l1-l2))\n  l4 = runif(1, min = 0, max = (1-l1-l2-l3))\n  l5 = 1-l1-l2-l3-l4\n  \n  lambdas = data.frame(order)\n  lambdas[1,2] = l1\n  lambdas[2,2] = l2\n  lambdas[3,2] = l3\n  lambdas[4,2] = l4\n  lambdas[5,2] = l5\n  \n  l1 = lambdas$V2[lambdas$order == 1]\n  l2 = lambdas$V2[lambdas$order == 2]\n  l3 = lambdas$V2[lambdas$order == 3]\n  l4 = lambdas$V2[lambdas$order == 4]\n  l5 = lambdas$V2[lambdas$order == 5]\n  \n  lambda_results[i,1] = l1\n  lambda_results[i,2] = l2\n  lambda_results[i,3] = l3\n  lambda_results[i,4] = l4\n  lambda_results[i,5] = l5\n  lambda_results[i,6] = 0\n  \n  i=i+1\n  \n}\n\ncolnames(lambda_results) = c(\"l1\", \"l2\", \"l3\", \"l4\", \"l5\", \"Accuracy\")\n\n\n\n#Lambda finder\n#----------------------------------\n\nz=1\n\nwhile (z <= nrow(lambda_results)) {\n  \n  l1 = lambda_results[z,1] \n  l2 = lambda_results[z,2] \n  l3 = lambda_results[z,3] \n  l4 = lambda_results[z,4]\n  l5 = lambda_results[z,5]\n  \n  \n  y = 1\n  counter = 0\n  eval = data.frame()\n  \n  while (y <= length(test_set)) {\n    \n    test = test_set[y]\n    test_split = strsplit(test, \" \")\n    len_new_word = length(test_split[[1]])\n    \n    \n    if (length(test_split[[1]]) > 4) {\n      \n      test = paste(test_split[[1]][len_new_word-3],test_split[[1]][len_new_word-2],\n                   test_split[[1]][len_new_word-1], test_split[[1]][len_new_word])\n      \n      test_split = strsplit(test, \" \")\n      \n    } \n    \n    \n    test_split_partial = test_split[[1]][1:length(test_split[[1]])-1]\n    \n    if (length(test_split_partial) == 4) {test_func=paste(test_split_partial[1],test_split_partial[2],test_split_partial[3],test_split_partial[4])}\n    if (length(test_split_partial) == 3) {test_func=paste(test_split_partial[1],test_split_partial[2],test_split_partial[3])}\n    if (length(test_split_partial) == 2) {test_func=paste(test_split_partial[1],test_split_partial[2])}\n    if (length(test_split_partial) == 1) {test_func=test_split_partial[1]}\n    \n    p_return = NWP(test_func, unknowns)\n    \n    prediction5 = p_return[grep(\"Five\", p_return$Prediction),]\n    prediction4 = p_return[grep(\"Quad\", p_return$Prediction),]\n    prediction3 = p_return[grep(\"Tri\", p_return$Prediction),]\n    prediction2 = p_return[grep(\"Bi\", p_return$Prediction),]\n    \n    if (length(grep(\"Uni\", p_return$Prediction)) != 0) {\n      \n      prediction1 = p_return[grep(\"Uni\", p_return$Prediction),]\n      \n    } else {prediction1 = p_return[grep(\"not found\", p_return$Prediction),]}\n    \n\n    \n    #Adjust Fivegrams\n    #----------------\n    \n    i=1\n    \n    if (prediction5$Prediction[1] != 'No Fivegram match') {\n      \n      while (i <= length(prediction5$Prediction)) {\n        \n        split = strsplit(prediction4$Prediction[i], \" \")\n        \n        prediction5$Adjust_prob[i] =  round(l5*(prediction5$Prob[i]) + \n                                              l4*(counts_dtm4$Prob[counts_dtm4$Word == paste(split[[1]][length(split[[1]])-4],split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                              l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                              l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                              l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n        \n        i=i+1\n      }\n      \n    } else {prediction5$Adjust_prob[1] = 0}\n    \n    \n    #Adjust Fourgrams\n    #----------------\n    \n    i=1\n    \n    if (prediction4$Prediction[1] != 'No Quadgram match') {\n      \n      while (i <= length(prediction4$Prediction)) {\n        \n        split = strsplit(prediction4$Prediction[i], \" \")\n        \n        prediction4$Adjust_prob[i] =  round(l4*(prediction4$Prob[i]) +\n                                              l3*(counts_dtm3$Prob[counts_dtm3$Word == paste(split[[1]][length(split[[1]])-3],split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) +\n                                              l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                              l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n        \n        i=i+1\n      }\n      \n    } else {prediction4$Adjust_prob[1] = 0}\n    \n    \n    \n    \n    #Adjust Trigrams\n    #----------------\n    \n    i=1\n    \n    if (prediction3$Prediction[1] != 'No Trigram match') {\n      \n      while (i <= length(prediction3$Prediction)) {\n        \n        split = strsplit(prediction3$Prediction[i], \" \")\n        \n        prediction3$Adjust_prob[i] =  round(l3*(prediction3$Prob[i]) + \n                                              l2*(counts_dtm2$Prob[counts_dtm2$Word == paste(split[[1]][length(split[[1]])-2],split[[1]][length(split[[1]])])]) + \n                                              l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n        \n        i=i+1\n      }\n    }  else {prediction3$Adjust_prob[1] = 0}\n    \n    \n    #Adjust Bigrams\n    #----------------\n    \n    i=1\n    \n    if (prediction2$Prediction[1] != 'No Bigram match') {\n      \n      while (i <= length(prediction2$Prediction)) {\n        \n        split = strsplit(prediction2$Prediction[i], \" \")\n        \n        prediction2$Adjust_prob[i] =  round(l2*(prediction2$Prob[i]) + \n                                              l1*(counts_dtm$Prob[counts_dtm$Word == split[[1]][length(split[[1]])]]), 3)\n        \n        i=i+1\n      } \n    } else {prediction2$Adjust_prob[1] = 0}\n    \n    \n    #Adjust Unigrams\n    #----------------\n    \n    i=1\n    \n    while (i <= length(prediction1$Prediction)) {\n      \n      #split = strsplit(prediction1$Prediction[i], \" \")\n      \n      prediction1$Adjust_prob[i] =  l1*(prediction1$Prob[i])\n      \n      i=i+1\n    }\n    \n    \n    \n    \n    #Re-merge,\n    \n    pred_all = rbind(prediction5, \n                     prediction4, \n                     prediction3, \n                     prediction2, \n                     prediction1)\n    \n    \n    pred_all = pred_all[order(-pred_all$Adjust_prob),]\n    pred_all\n    \n    pred_all_split = strsplit(pred_all$Prediction, \" | \")\n    \n    \n    \n    \n    #Make the top 3 results unique,\n    \n    if (pred_all_split[[1]][length(pred_all_split[[1]])] == pred_all_split[[2]][length(pred_all_split[[2]])]) {\n      \n      pred_all = pred_all[-2,]\n      \n    }\n    \n    pred_all_split = strsplit(pred_all$Prediction, \" | \")\n    \n    if (pred_all_split[[2]][length(pred_all_split[[2]])] == pred_all_split[[3]][length(pred_all_split[[3]])]) {\n      \n      pred_all = pred_all[-3,]\n      \n    }\n    \n    \n    \n    p1 = pred_all[1,1]\n    p1 = strsplit(p1, \" | \")\n    p1 = p1[[1]][length(p1[[1]])]\n    \n    if (dim(pred_all)[1] >= 2) {\n      \n      p2 = pred_all[2,1]\n      p2 = strsplit(p2, \" | \")\n      p2 = p2[[1]][length(p2[[1]])]\n      \n    } else {p2 = \" \"}\n    \n    if (dim(p_return)[1] >= 3) {\n      \n      p3 = pred_all[3,1]\n      p3 = strsplit(p3, \" | \")\n      p3 = p3[[1]][length(p3[[1]])]\n      \n    } else {p3 = \" \"}\n    \n    \n    eval[y,1] = test\n    eval[y,2] = p1\n    eval[y,3] = p2\n    eval[y,4] = p3\n    \n    if (test_split[[1]][length(test_split[[1]])] == eval[y,2] | \n        test_split[[1]][length(test_split[[1]])] == eval[y,3] | \n        test_split[[1]][length(test_split[[1]])] == eval[y,4]) {counter = counter+1}\n    \n    print(paste(\"z\",z, \":\", y/length(test_set)*100))\n    y=y+1\n    \n  }\n  \n  \n  accuracy = counter / length(test_set)\n  lambda_results[z,6] = accuracy*100\n  z=z+1\n  \n}\n\n\nbest = head(lambda_results[order(-lambda_results$Accuracy),],1)\n\nl1 = best[1]\nl2 = best[2]\nl3 = best[3]\nl4 = best[4]\nl5 = best[5]\n\nlambdas_chosen = data.frame(l1,l2,l3,l4,l5)\n\nsave(lambdas_chosen, file = \"lambdas_chosen.RData\")\nload(\"lambdas_chosen.RData\")\n\n\n",
    "created" : 1458832326586.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "85521556",
    "id" : "B2131FBF",
    "lastKnownWriteTime" : 1461057844,
    "path" : "C:/Users/rob.harrand/Desktop/WORK/Coursera/Module 10 - Capstone/Interpolation.R",
    "project_path" : "Interpolation.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}