input_s
input_s = head(input_s[[1]],-1)
input_s
runApp("Shiny")
runApp("Shiny")
u1
load("pred_output1.RData")
u1 = strsplit(pred_output1, "'")
u1 = u1[[1]][2]
u1
load("pred_output1.RData")
u1 = strsplit(pred_output1, "'")
u1
u1 = u1[[1]][2]
input_s = 'the amazing chilld'
input_s = strsplit(input_s, " ")
input_s
input_s = head(input_s[[1]],-1)
input_s
u1 = paste(input_s, u1)
u1
input_s
u1
load("pred_output1.RData")
u1 = strsplit(pred_output1, "'")
u1 = u1[[1]][2]
u1
input_s = 'the amazing chilld'
input_s = strsplit(input_s, " ")
input_s = head(input_s[[1]],-1)
input_s
u1
u_final = paste(input_s, u1)
u_final
load("pred_output1.RData")
u1 = strsplit(pred_output1, "'")
u1 = u1[[1]][2]
input_s = 'the amazing chilld'
input_s = strsplit(input_s, " ")
input_s = head(input_s[[1]],-1)
input_s
u1
paste(input_s, u1)
input_s
paste(input_s)
input_s[1]
input_s = paste(input_s[1],input_s[2])
u1 = paste(input_s, u1)
u1
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
input_s = "the amazing wonderful outstanding chilld"
input_s = strsplit(input_s, " ")
input_s
input_s = head(input_s[[1]],-1)
input_s
input_s = paste(input_s, collapse = "")
input_s
input_s = "the amazing wonderful outstanding chilld"
input_s = strsplit(input_s, " ")
input_s = head(input_s[[1]],-1)
input_s = paste(input_s, collapse = " ")
input_s
runApp("Shiny")
unknowns[1]
View(unknowns)
pred_output1 = unknowns[1,1]
pred_output1
runApp("Shiny")
as.character(unknowns[1,1])
runApp("Shiny")
runApp("Shiny")
pred_output1 = paste("Prediction 1:",as.character(unknowns[1,1]))
pred_output1
test_word = "the amazing chilld"
save(test_word, file = "Sentence.RData")
load("Sentence.RData")
test_word
sentence = strsplit(test_word, " ")
sentence
head(sentence,-1)
sentence = head(sentence[[1]],-1)
sentence
sentence = paste(sentence, collapse = " ")
sentence
runApp("Shiny")
pred_output1 = paste("Prediction 1:", sentence, "|", as.character(unknowns[1,1]))
pred_output1
runApp("Shiny")
library(shiny)
runApp("Shiny")
install.packages('rsconnect')
install.packages("rsconnect")
library(rsconnect)
rsconnect::setAccountInfo(name='robharrand',
token='2B220EC29DBE52B0573AAD7F50F00A27',
secret='<SECRET>')
rsconnect::setAccountInfo(name='robharrand', token='2B220EC29DBE52B0573AAD7F50F00A27', secret='U2kdqwz04J3smyL+fXLwRcJ9VP2GteuCQQBiODs9')
runApp("Shiny")
library(shiny)
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
load(lambdas_chosen)
load(file = "lambdas_chosen.RData")
getwd()
#load(file = "\\Shiny\\lambdas_chosen.RData")
load("lambdas_chosen.RData")
load(file = "kn_2pc.RData")
View(lambdas_chosen)
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
runApp("Shiny")
library(shiny)
runApp("Shiny")
load("Sentence.RData")
load("Sentence.RData")
sentence = strsplit(test_word, " ")
test_word = "Leaving EU would liberate UK"
sentence = strsplit(test_word, " ")
sentence = head(sentence[[1]],-1)
sentence = paste(sentence, collapse = " ")
pred_output1 = paste("Prediction 1:", sentence, "|", as.character(unknowns[1,1]))
test_word = "Leaving EU would liberate UK"
sentence = strsplit(test_word, " ")
sentence = head(sentence[[1]])
sentence = paste(sentence, collapse = " ")
runApp("Shiny")
runApp("Shiny")
load(file = "counts_dtm_2pc.RData")
View(counts_dtm)
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
counts_dtm[,1]
counts_dtm[,1] = tm_map(counts_dtm[,1], removeWords, swearwords$V1)
counts_dtm[,1] = as.character(counts_dtm[,1])
counts_dtm[,1] = tm_map(counts_dtm[,1], removeWords, swearwords$V1)
options( java.parameters = "-Xmx4g" ) #Set memory options
#Load libraries,
library("tm")
library("SnowballC")
library("stringr")
library("readr")
library('stringdist')
library(RWeka)
en_blogs = read_lines("texts/en_US/en_US.blogs.txt")
en_tweets = readLines("texts/en_US/en_US.twitter.txt", skipNul = T)
en_news = read_lines("texts/en_US/en_US.news.txt")
dict = read_lines("texts/Dict.txt")
prop = 0.02
set.seed(100)
en_blogs_sample = sample(en_blogs, (length(en_blogs)*prop))
set.seed(100)
en_tweets_sample = sample(en_tweets, (length(en_tweets)*prop))
set.seed(100)
en_news_sample = sample(en_news, (length(en_news)*prop))
en_blogs_sample = iconv(en_blogs_sample, "latin1", "ASCII", "")
en_tweets_sample = iconv(en_tweets_sample, "latin1", "ASCII", "")
en_news_sample = iconv(en_news_sample, "latin1", "ASCII", "")
rm(en_blogs)
rm(en_tweets)
rm(en_news)
all_sample = c(en_blogs_sample, en_tweets_sample, en_news_sample)
all_sample_corpus = Corpus(VectorSource(list(all_sample)))
rm(en_blogs_sample)
rm(en_tweets_sample)
rm(en_news_sample)
rm(all_sample)
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
all_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)
rm(swearwords)
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
all_sample_corpus
terms(all_sample_corpus)
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
all_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)
options( java.parameters = "-Xmx4g" ) #Set memory options
#Load libraries,
library("tm")
library("SnowballC")
library("stringr")
library("readr")
library('stringdist')
library(RWeka)
#Load text files,
en_blogs = read_lines("texts/en_US/en_US.blogs.txt")
en_tweets = readLines("texts/en_US/en_US.twitter.txt", skipNul = T)
en_news = read_lines("texts/en_US/en_US.news.txt")
dict = read_lines("texts/Dict.txt")
prop = 0.02
set.seed(100)
en_blogs_sample = sample(en_blogs, (length(en_blogs)*prop))
set.seed(100)
en_tweets_sample = sample(en_tweets, (length(en_tweets)*prop))
set.seed(100)
en_news_sample = sample(en_news, (length(en_news)*prop))
en_blogs_sample = iconv(en_blogs_sample, "latin1", "ASCII", "")
en_tweets_sample = iconv(en_tweets_sample, "latin1", "ASCII", "")
en_news_sample = iconv(en_news_sample, "latin1", "ASCII", "")
rm(en_blogs)
rm(en_tweets)
rm(en_news)
all_sample = c(en_blogs_sample, en_tweets_sample, en_news_sample)
all_sample_corpus = Corpus(VectorSource(list(all_sample)))
rm(en_blogs_sample)
rm(en_tweets_sample)
rm(en_news_sample)
rm(all_sample)
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
all_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)
all_sample_corpus = tm_map(all_sample_corpus, removePunctuation)
all_sample_corpus = tm_map(all_sample_corpus, content_transformer(tolower))
all_sample_corpus = tm_map(all_sample_corpus, removeNumbers)
all_sample_corpus = tm_map(all_sample_corpus, stripWhitespace)
all_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)
options( java.parameters = "-Xmx4g" ) #Set memory options
#Load libraries,
library("tm")
library("SnowballC")
library("stringr")
library("readr")
library('stringdist')
library(RWeka)
#Load text files,
en_blogs = read_lines("texts/en_US/en_US.blogs.txt")
en_tweets = readLines("texts/en_US/en_US.twitter.txt", skipNul = T)
en_news = read_lines("texts/en_US/en_US.news.txt")
dict = read_lines("texts/Dict.txt")
prop = 0.02
set.seed(100)
en_blogs_sample = sample(en_blogs, (length(en_blogs)*prop))
set.seed(100)
en_tweets_sample = sample(en_tweets, (length(en_tweets)*prop))
set.seed(100)
en_news_sample = sample(en_news, (length(en_news)*prop))
en_blogs_sample = iconv(en_blogs_sample, "latin1", "ASCII", "")
en_tweets_sample = iconv(en_tweets_sample, "latin1", "ASCII", "")
en_news_sample = iconv(en_news_sample, "latin1", "ASCII", "")
rm(en_blogs)
rm(en_tweets)
rm(en_news)
all_sample = c(en_blogs_sample, en_tweets_sample, en_news_sample)
all_sample_corpus = Corpus(VectorSource(list(all_sample)))
rm(en_blogs_sample)
rm(en_tweets_sample)
rm(en_news_sample)
rm(all_sample)
#Tidy
#---------------------------------------
#Remove punctuation â€“ replace punctuation marks with a space
all_sample_corpus = tm_map(all_sample_corpus, removePunctuation)
all_sample_corpus = tm_map(all_sample_corpus, content_transformer(tolower))
all_sample_corpus = tm_map(all_sample_corpus, removeNumbers)
all_sample_corpus = tm_map(all_sample_corpus, stripWhitespace)
#Remove swearwords,
swearwords = read.delim("swearWords.txt", sep = "\n", header = F)
all_sample_corpus = tm_map(all_sample_corpus, removeWords, swearwords$V1)
rm(swearwords)
OnegramTokenizer = function(x) WordTokenizer(x)
dtm = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = OnegramTokenizer, wordLengths = c(1, Inf)))
freq = colSums(as.matrix(dtm))
counts_dtm = data.frame(Word=names(freq), Freq=freq)
counts_dtm = counts_dtm[counts_dtm$Freq>2,] #Pruning
counts_dtm$Word = droplevels(counts_dtm$Word)
save(counts_dtm, file = "counts_dtm.RData")
save(freq, file = "freq.RData")
counts_dtm$Word
counts_dtm
counts_dtm[counts_dtm$Word == 'the']
counts_dtm[counts_dtm$Word == 'the',]
counts_dtm[counts_dtm$Word == 'fuk',]
rm(dtm)
rm(counts_dtm)
rm(freq)
TwogramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
dtm2 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = TwogramTokenizer))
freq2 = colSums(as.matrix(dtm2))
counts_dtm2 = data.frame(Word=names(freq2), Freq=freq2)
counts_dtm2 = counts_dtm2[counts_dtm2$Freq>2,] #Pruning
counts_dtm2$Word = droplevels(counts_dtm2$Word)
save(counts_dtm2, file = "counts_dtm2.RData")
save(freq2, file = "freq2.RData")
rm(dtm2)
rm(counts_dtm2)
rm(freq2)
ThreegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
dtm3 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = ThreegramTokenizer))
freq3 = colSums(as.matrix(dtm3))
counts_dtm3 = data.frame(Word=names(freq3), Freq=freq3)
counts_dtm3 = counts_dtm3[counts_dtm3$Freq>2,] #Pruning
counts_dtm3$Word = droplevels(counts_dtm3$Word)
save(counts_dtm3, file = "counts_dtm3.RData")
save(freq3, file = "freq3.RData")
rm(dtm3)
rm(counts_dtm3)
rm(freq3)
FourgramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
dtm4 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FourgramTokenizer))
freq4 = colSums(as.matrix(dtm4))
counts_dtm4 = data.frame(Word=names(freq4), Freq=freq4)
counts_dtm4 = counts_dtm4[counts_dtm4$Freq>2,] #Pruning
counts_dtm4$Word = droplevels(counts_dtm4$Word)
save(counts_dtm4, file = "counts_dtm4.RData")
save(freq4, file = "freq4.RData")
rm(dtm4)
rm(counts_dtm4)
rm(freq4)
FivegramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
dtm5 = DocumentTermMatrix(all_sample_corpus, control = list(tokenize = FivegramTokenizer))
freq5 = colSums(as.matrix(dtm5))
counts_dtm5 = data.frame(Word=names(freq5), Freq=freq5)
counts_dtm5 = counts_dtm5[counts_dtm5$Freq>2,] #Pruning
counts_dtm5$Word = droplevels(counts_dtm5$Word)
save(counts_dtm5, file = "counts_dtm5.RData")
save(freq5, file = "freq5.RData")
rm(dtm5)
rm(counts_dtm5)
rm(freq5)
save(all_sample_corpus, file = "all_sample_corpus.RData")
Uni_length = sum(counts_dtm$Freq)
counts_dtm$Prob = counts_dtm$Freq / Uni_length
#Bigram probabiltiies
counts_dtm2$Prob = 0
i=1
while (i <= length(counts_dtm2$Word)) {
uni_count = counts_dtm$Freq[counts_dtm$Word == (strsplit(as.character(counts_dtm2$Word[i]), " ")[[1]][1])]
bi_count = counts_dtm2$Freq[i]
counts_dtm2$Prob[i] = bi_count / uni_count
i=i+1
print((i/length(counts_dtm2$Word)*100))
}
#Trigram probabiltiies
#Maximum likelihood estimate for a trigram, P(wi | wi-1 wi-2 ) = count(wi, wi-1, wi-2 ) / count(wi-1, wi-2 )
#For example, P(Park | The car) = the number of trigrams of 'The car park' divided by the number of bigrams of 'The car'
counts_dtm3$Prob = 0
i=1
while (i <= length(counts_dtm3$Word)) {
bi_count = counts_dtm2$Freq[counts_dtm2$Word == paste(strsplit(as.character(counts_dtm3$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm3$Word[i]), " ")[[1]][2])]
tri_count = counts_dtm3$Freq[i]
counts_dtm3$Prob[i] = tri_count / bi_count
i=i+1
print((i/length(counts_dtm3$Word)*100))
}
#Quadgram probabiltiies
counts_dtm4$Prob = 0
i=1
while (i <= length(counts_dtm4$Word)) {
tri_count = counts_dtm3$Freq[counts_dtm3$Word == paste(strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][2],
strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][3])]
quad_count = counts_dtm4$Freq[i]
counts_dtm4$Prob[i] = quad_count / tri_count
i=i+1
print((i/length(counts_dtm4$Word)*100))
}
#Fivegram probabiltiies
counts_dtm5$Prob = 0
i=1
while (i <= length(counts_dtm5$Word)) {
quad_count = counts_dtm4$Freq[counts_dtm4$Word == paste(strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][2],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][3],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][4])]
five_count = counts_dtm5$Freq[i]
counts_dtm5$Prob[i] = five_count / quad_count
i=i+1
print((i/length(counts_dtm5$Word)*100))
}
#Simple interpolation (lambdas are static for all cases)
#------------------------------------------------------------------
save(counts_dtm, file = "counts_dtm.RData")
save(counts_dtm2, file = "counts_dtm2.RData")
save(counts_dtm3, file = "counts_dtm3.RData")
save(counts_dtm4, file = "counts_dtm4.RData")
save(counts_dtm5, file = "counts_dtm5.RData")
load("counts_dtm.RData")
load("counts_dtm2.RData")
load("counts_dtm3.RData")
load("counts_dtm4.RData")
load("counts_dtm5.RData")
#Maximum likelihood estimate for a bigram, P(Wi | Wi-1) = count(Wi-1, Wi) / count(Wi-1)
#For example, P(Park | Car) = the number of bigrams of 'Car Park' divided by the number of unigrams of 'Car'
#Unigram probabilities
Uni_length = sum(counts_dtm$Freq)
counts_dtm$Prob = counts_dtm$Freq / Uni_length
#Bigram probabiltiies
counts_dtm2$Prob = 0
i=1
while (i <= length(counts_dtm2$Word)) {
uni_count = counts_dtm$Freq[counts_dtm$Word == (strsplit(as.character(counts_dtm2$Word[i]), " ")[[1]][1])]
bi_count = counts_dtm2$Freq[i]
counts_dtm2$Prob[i] = bi_count / uni_count
i=i+1
print((i/length(counts_dtm2$Word)*100))
}
#Trigram probabiltiies
#Maximum likelihood estimate for a trigram, P(wi | wi-1 wi-2 ) = count(wi, wi-1, wi-2 ) / count(wi-1, wi-2 )
#For example, P(Park | The car) = the number of trigrams of 'The car park' divided by the number of bigrams of 'The car'
counts_dtm3$Prob = 0
i=1
while (i <= length(counts_dtm3$Word)) {
bi_count = counts_dtm2$Freq[counts_dtm2$Word == paste(strsplit(as.character(counts_dtm3$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm3$Word[i]), " ")[[1]][2])]
tri_count = counts_dtm3$Freq[i]
counts_dtm3$Prob[i] = tri_count / bi_count
i=i+1
print((i/length(counts_dtm3$Word)*100))
}
#Quadgram probabiltiies
counts_dtm4$Prob = 0
i=1
while (i <= length(counts_dtm4$Word)) {
tri_count = counts_dtm3$Freq[counts_dtm3$Word == paste(strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][2],
strsplit(as.character(counts_dtm4$Word[i]), " ")[[1]][3])]
quad_count = counts_dtm4$Freq[i]
counts_dtm4$Prob[i] = quad_count / tri_count
i=i+1
print((i/length(counts_dtm4$Word)*100))
}
#Fivegram probabiltiies
counts_dtm5$Prob = 0
i=1
while (i <= length(counts_dtm5$Word)) {
quad_count = counts_dtm4$Freq[counts_dtm4$Word == paste(strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][1],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][2],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][3],
strsplit(as.character(counts_dtm5$Word[i]), " ")[[1]][4])]
five_count = counts_dtm5$Freq[i]
counts_dtm5$Prob[i] = five_count / quad_count
i=i+1
print((i/length(counts_dtm5$Word)*100))
}
save(counts_dtm, file = "counts_dtm.RData")
save(counts_dtm2, file = "counts_dtm2.RData")
save(counts_dtm3, file = "counts_dtm3.RData")
save(counts_dtm4, file = "counts_dtm4.RData")
save(counts_dtm5, file = "counts_dtm5.RData")
#Kneser-Ney smoothing
#---------------------------------------------
#First, we need the corpus as bigrams (not just the unique bigrams, but the whole lot),
bis = as.data.frame(strsplit(as.character(counts_dtm2$Word), " "))
bis = as.data.frame(t(bis))
bis = cbind(bis, counts_dtm2$Freq)
colnames(bis) = c("Word1", "Word2", "Freq")
#Next, we need to know what are the most common 2nd words in the bigrams
most_bi = sort(bis$Freq, decreasing = T)
most_bis = bis[bis$Freq %in% most_bi,]
most_bis = most_bis[order(-most_bis$Freq),]
most_bis = droplevels(most_bis)
unique_w2 = unique(most_bis$Word2)
unique_w1 = unique(most_bis$Word1)
length_element_w1 = length(unique_w1)
#Then, from these common endings, we need to know how many bigrams each completes
#Finally, we work out the continuum probability based upon this
kn = data.frame()
i=1
while (i <= length(unique_w2)) {
sum_element = sum(most_bis$Freq[most_bis$Word2 == unique_w2[i]])
length_element_w2 = length(most_bis$Word1[most_bis$Word2 == unique_w2[i]])
prob_element = length_element_w2 / length_element_w1 #No. of word types seen to precede / no. of words preceding all words
kn[i,1] = unique_w2[i]
kn[i,2] = sum_element
kn[i,3] = length_element_w2
kn[i,4] = prob_element
i=i+1
print(i/length(unique_w2)*100)
}
colnames(kn) = c("2nd Word", "Freq", "No. of bigrams it completes", "Pcont")
kn = kn[order(-kn$Pcont),]
save(kn, file = "kn.RData")
getwd()
runApp("Shiny")
runApp("Shiny")
library(rsconnect)
rsconnect::setAccountInfo(name='robharrand', token='2B220EC29DBE52B0573AAD7F50F00A27', secret='U2kdqwz04J3smyL+fXLwRcJ9VP2GteuCQQBiODs9')
runApp("Shiny")
author: Tejash Panchal
