{
    "contents" : "---\ntitle: \"Predicting Future Movies - A Bayesian Approach\"\noutput: \n  html_document: \n    fig_height: 4\n    highlight: pygments\n    theme: spacelab\n---\n\n## Setup\n\n### Load packages\n\n```{r load-packages, message = FALSE, warning = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(statsr)\nlibrary(BAS)\nlibrary(MASS)\n```\n\n### Load data\n\n```{r load-data}\nload(\"movies.Rdata\")\n```\n\n\n* * *\n\n## Part 1: Data\n\nThe data is taken from IMDB (The Internet Movie Database) and Rotten Tomatoes, and represents a random sample from their archives. There is insufficient information provided to know if this randomisation was done well, so the data should be treated with caution. It is also unclear what type of movies tend to appear on these websites. If good and bad movies are not equally represented, then the sample may be biased.\n\n* * *\n\n## Part 2: Data manipulation\n\n```{r}\n#Use the 'mutate' function and 'ifelse' function to create the new variables,\n\nmovies <- mutate(movies, feature_film = ifelse(movies$title_type == 'Feature Film', 'Yes', 'No'))\nmovies$feature_film = as.factor(movies$feature_film)\n\nmovies <- mutate(movies, drama = ifelse(movies$genre == 'Drama', 'Yes', 'No'))\nmovies$drama = as.factor(movies$drama)\n\nmovies <- mutate(movies, mpaa_rating_R = ifelse(movies$mpaa_rating == 'R', 'Yes', 'No'))\nmovies$mpaa_rating_R = as.factor(movies$mpaa_rating_R)\n\nmovies <- mutate(movies, oscar_season = ifelse(movies$thtr_rel_month %in% c(10:12), 'Yes', 'No'))\nmovies$oscar_season = as.factor(movies$oscar_season)\n\nmovies <- mutate(movies, summer_season = ifelse(movies$thtr_rel_month %in% c(5:8), 'Yes', 'No'))\nmovies$summer_season = as.factor(movies$summer_season)\n```\n\n* * *\n\n## Part 3: Exploratory data analysis\n\nLet's take a look at the distribution of the audience scores,\n\n```{r, fig.width=5, fig.height=5}\nhist(movies$audience_score, breaks = 100, main = \"Histogram of audience scores\", xlab = \"Audience score\")\n```\n\nThis doesn't seem to show any particular structure. How about the spread of movie genres?,\n\n```{r, fig.width=5, fig.height=5}\ntab = table(movies$genre)\ntab = tab[order(-tab)]\npar(mar=c(12.1, 4.1, 4.1, 2.1))\nbarplot(tab, las=2, ylab = 'No. of movies', main = 'Genre split of the data', col = 'orange')\n```\n\nThere is a clear dominance of dramas.  \n\nNext let's take a look at the boxplots for the new variables (feature film, drama, \nmpaa rating R, oscar season and summer season) vs audience score. All of these variables are binary variables, and so lends themselves to\nrepresentation by boxplots,\n\n```{r, fig.width=8, fig.height=8}\npar(mar=c(5.1, 4.1, 4.1, 2.1))\npar(mfrow = c(2,3))\n\nboxplot(movies$audience_score ~ movies$feature_film, \n        main = \"Audience score \\n vs Feature Film\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$drama, \n        main = \"Audience score \\n vs Drama\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$mpaa_rating_R, \n        main = \"Audience score \\n vs MPAA Rating R\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$oscar_season, \n        main = \"Audience score \\n vs Oscar Season\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$summer_season, \n        main = \"Audience score \\n vs Summer Season\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\np <- ggplot(movies, aes(feature_film, audience_score)) + geom_violin(aes(fill = feature_film)) + geom_boxplot(width=.2)\n\n\n```\n\nThe central line on each box is the median for that sub-group. A rule-of-thumb for significance is for the medians to be outside of the inter-quartile range (the coloured section) of the neighbouring box. As you can see, only 'feature film' fits this description. We can be more rigorous with a Wilcoxon signed-rank test, which is a non-parametric hypothesis test to check for differences between groups. This is from the frequentist paradigm, but worth trying for this early, exploratory phase,\n\n```{r}\nw1 = format(wilcox.test(movies$audience_score ~ movies$feature_film)$p.value * 100, nsmall = 3)\nw2 = format(wilcox.test(movies$audience_score ~ movies$drama)$p.value * 100, nsmall = 3)\nw3 = format(wilcox.test(movies$audience_score ~ movies$mpaa_rating_R)$p.value * 100, nsmall = 3)\nw4 = format(wilcox.test(movies$audience_score ~ movies$oscar_season)$p.value * 100, nsmall = 3)\nw5 = format(wilcox.test(movies$audience_score ~ movies$summer_season)$p.value * 100, nsmall = 3)\n\nw = data.frame(\"Variable\" = c(\"Feature film\", \"Drama\", \"MPAA Rating R\", \"Oscar Season\", \"Summer Season\"), \"p-values\" = c(w1,w2,w3,w4,w5))\ncolnames(w) = c(\"Variable\", \"p-value (%)\")\n\nw\n```\n\nThis shows that feature film and drama appear to have some significance. For completeness, let's take a quick look at the remaining categorical variables,\n\n```{r, fig.width=8, fig.height=8}\npar(mfrow = c(2,3))\n\nboxplot(movies$audience_score ~ movies$best_pic_nom,\n        main = \"Audience score \\n vs Best pic nomination\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$best_actor_win,\n        main = \"Audience score \\n vs Best actor win\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$best_actress_win,\n        main = \"Audience score \\n vs Best actress win\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$best_dir_win, \n        main = \"Audience score \\n vs Best director win\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n\nboxplot(movies$audience_score ~ movies$top200_box,\n        main = \"Audience score \\n vs Top 200 box-office\", ylab = \"Audience score\", col = c(\"palevioletred1\", \"light green\"))\n```\n\n'Best picture nomination', 'top 200 box-office' and to a smaller extent, 'best director win' seem to have some difference. Finally, let's look at the numerical variables,\n\n```{r, fig.width=8, fig.height=8}\npar(mfrow = c(2,3))\n\nplot(movies$runtime, movies$audience_score, xlab = \"Run time\", ylab = \"Audience score\")\nplot(movies$thtr_rel_year, movies$audience_score, xlab = \"Release year\", ylab = \"Audience score\")\nplot(movies$imdb_rating, movies$audience_score, xlab = \"IMDB Rating\", ylab = \"Audience score\")\nplot(movies$imdb_num_votes, movies$audience_score, xlab = \"No. of IMDB votes\", ylab = \"Audience score\")\nplot(movies$critics_score, movies$audience_score, xlab = \"Critics scores\", ylab = \"Audience score\")\n```\n\nThere is a clear correlation between audience score and both IMDB rating and critics score, which makes intuitive sense. We should expect these to be important in the eventual model.\n\n* * *\n\n## Part 4: Modeling\n\nFirst, let's create a predicative model based upon a number of variables. Note that 'runtime' has one NA value, which is removed first,\n\n```{r}\nex = is.na(movies$runtime)\nmovies_no_na = movies[!ex,]\n\nmovie_model = lm(audience_score ~ feature_film + \n                                  drama + \n                                  runtime + \n                                  mpaa_rating_R + \n                                  thtr_rel_year + \n                                  oscar_season + \n                                  summer_season +\n                                  imdb_rating + \n                                  imdb_num_votes + \n                                  critics_score + \n                                  best_pic_nom + \n                                  best_pic_win + \n                                  best_actor_win + \n                                  best_actress_win + \n                                  best_dir_win + \n                                  top200_box, \n                                  data = movies_no_na)\n\ns = summary(movie_model)\ns$coefficients[order(s$coefficients[,1]), decreasing = T]\n```\n\nAs suspected, IMDB rating is having a dominant effect.  \n\nWhat about the Bayesian information criteria? ...\n\n```{r}\nBIC(movie_model)\n```\n\nOut of interest, what would automatic model selection based upon AIC (Akaike information criterion) look like? (Note that I've hidden the raw output as it was huge, and instead have just shown the ANOVA results),\n\n```{r, echo=F, results=\"hide\"}\nk = log(length(movies$audience_score))\nst = stepAIC(movie_model, k = k)\n```\n\n```{r}\nst$anova\n```\n\nwhat about the final model? ...\n\n```{r}\nst\n```\n\nSo this is showing that just 3 predictors; runtime, IMDB rating and Critics score can be used to make a sensible prediction.  \n\nNext let's create a Bayesian model using a uniform distribution as the prior, as I have no prior knowledge in the area of movie scores,\n\n```{r}\nbma_movies = bas.lm(audience_score ~ feature_film + \n                                  drama + \n                                  runtime + \n                                  mpaa_rating_R + \n                                  thtr_rel_year + \n                                  oscar_season + \n                                  summer_season +\n                                  imdb_rating + \n                                  imdb_num_votes + \n                                  critics_score + \n                                  best_pic_nom + \n                                  best_pic_win + \n                                  best_actor_win + \n                                  best_actress_win + \n                                  best_dir_win + \n                                  top200_box, \n                                  data = movies_no_na,\n                                  prior = \"BIC\", \n                                  modelprior = uniform())\n\n```\n\nThe following shows the probability that each variable is included in a model,\n\n```{r}\nbma_movies\n```\n\nThis shows, for example, that IMDB is always included. Next, let's look at the top 5 models,\n\n```{r}\nsummary(bma_movies)\n```\n\nNotice that the top model has the same 3 predictors as found using the automatic AIC function above; runtime, IMDB rating and Critics score. The next best model with a Bayes Factor and Posterior probability almost the same as the top model uses just IMDB rating and critics score, dropping runtime.  \nLet's have a look at a visual representation of this,\n\n```{r, fig.width=5, fig.height=5}\nimage(bma_movies, rotate = F)\n```\n\nNotice how dominant IMDB rating and critics score is across the possible models (black = not included).  \n\nNext, let's check the posterior means of the coefficients,\n\n```{r}\nc = coefficients(bma_movies)\n```\n\nComparing the coefficents between the frequentist and Bayesian models,\n\n```{r, fig.width=5, fig.height=5, warning=F}\npar(mfrow = c(1,1))\nplot(s$coefficients[,1], c$postmean, log = \"xy\", main = \"Frequentist vs Bayesian model parameter values\", \n     xlab = \"Frequentist (log)\", ylab = \"Baysian posterior mean (log)\", col = 'blue', pch = 16)\nabline(lm(s$coefficients[,1] ~ c$postmean), col = \"red\")\n```\n\nThis shows some disagreement. Let's look at the posterior distributions of some variables of interest, namely, Runtime, IMDB Rating and Critics score. I've also included Drama as an example of a variable that isn't of particular interest,\n\n```{r, fig.width=8, fig.height=8}\npar(mfrow = c(2,2))\nplot(c, subset = c(4,9,11,3), ask=FALSE)\n```\n\nWe can see high probabilities of the first 3 distributions, with only Runtime crossing zero. In contrast, Drama has a very low probability and a posterior distribution that clearly includes zero. Note also the large vertical bar for Drama, indicating a high posterior probability that the coefficient is zero.  \n\nFinally, let's check the residuals of the model,\n\n```{r fig.width=5, fig.height=5}\nplot(bma_movies, which=1, add.smooth = F)\n```\n\nThis shows 3 movies that may be outliers, although they don't appear to be overly extreme, and there are only 3 out of 650 observations.  More concerning is that the plot shows some evidence of heteroscedasticity, as the residuals get larger as the prediction moves from large to small. This suggests that the model has room for improvement.\n\n* * *\n\n## Part 5: Prediction\n\nFirst, let's create a new, parsimonious model using just runtime, IMDB rating and Critics score,\n\n```{r}\nbma_movies_simple = bas.lm(audience_score ~ runtime + \n                                  imdb_rating + \n                                  critics_score,\n                                  data = movies_no_na,\n                                  prior = \"BIC\", \n                                  modelprior = uniform())\n\n```\n\nFor the new data, I've taken information from IMDB for the film 'Star Trek Beyond' and placed it into a data-frame,\n\n```{r}\nmovies_test = data.frame(audience_score = 0,\n                         runtime = 120,\n                         imdb_rating = 7.5, \n                         critics_score = 68)\n```\n\n\nNow let's make a prediction using HPM for the highest prediction model (this is to select the simple model that we've created, rather than take other approaches such as averaging the alternative models),\n\n```{r, fig.width=5, fig.height=5}\n\n#Make a prediction,\nbma_movies_pred_simple = predict(bma_movies_simple, newdata = movies_test, se.fit=TRUE, estimator = 'HPM')\nbma_movies_pred_simple$fit[1]\n\nrotten = 84\nimdb = 7.5\n\npar(mfrow = c(1,1))\nplot(movies$imdb_rating, movies$audience_score, xlim = c(0,10), ylim = c(0,100), \n     col = 'black', main = \"IMDB Rating vs Audience Score\", xlab = \"IMDB Rating\", ylab = \"Audience Score\", pch=16)\n\npoints(x = imdb, y = bma_movies_pred_simple$fit[1], col = 'red', pch = 19, cex = 2.5)\npoints(x = imdb, y = rotten, col = 'green', pch = 19, cex = 2.5)\n```\n\nThis plot shows IMDB score vs audience score, the predicted score (in red) and the actual score taken from Rotten Tomartoes (in green).  \n\nNow let's work out the credible intervals for the prediction,\n\n```{r, results='hide'}\nci = confint(bma_movies_pred_simple, estimator = 'HPM')\nopt_hpm = which.max(bma_movies_pred_simple$fit)\n```\n\n```{r}\nci[opt_hpm,]\n```\n\nOne final thing to look at. The question here is 'what attributes make a movie popular?', and we've ended up with a model that uses the IMDB rating and critics score. However, they are not attributes intrinsic to the movie. Instead, *attributes intrinsic to the movie lead to the IMDB and critics scores*. Therefore, it could be argued that these should be left out of the model. That would instead give the following,\n\n```{r, fig.width=5, fig.height=5}\nbma_movies_features = bas.lm(audience_score ~ feature_film + \n                                  drama + \n                                  runtime + \n                                  mpaa_rating_R + \n                                  thtr_rel_year + \n                                  oscar_season + \n                                  summer_season +\n                                  best_pic_nom + \n                                  best_pic_win + \n                                  best_actor_win + \n                                  best_actress_win + \n                                  best_dir_win,\n                                  data = movies_no_na,\n                                  prior = \"BIC\", \n                                  modelprior = uniform())\n\n\nimage(bma_movies_features, rotate = F)\n```\n\nNow we see actual movie attributes dominating, such as feature film, drama and runtime. This argument could be extended further by asking 'is a best picture nomination what makes a movie popular, or does a popular movie lead to a best picture nomination?'\n\n* * *\n\n## Part 6: Conclusion\n\nWe've created several new variables for the movies dataset, and then looked at frequentist and Bayesian models created using the data. In conclusion, many of the variables have little influence on the final model, including all the new variables created. In the end, just the IMDB rating, critics score and runtime were needed to create a simple, effective model (note that the first two will almost certainly have a degree of colinearity).  \n\nThe sample appears to be a reasonable representation of the general catalogue of movies that exist. However, one shortcoming is in the range of genres. The largest category by far is Drama, with Sci-fi having just 9 movies. Given that this dataset spans the period of 1970 to 2014, that's about one sci-fi movie for every five years. A better balance of genres would potentially make for better predictions.  \n\nThe overall size of the database could also be increased, as-well-as having more movies from foreign markets.  \n\nTo further this work, I would suggest digging deeper into the precise nature of the question in order to refine the attributes that are used. Having a model that correctly predicts movie score based upon variables that aren't intrinsic to the movie, but are instead features created by the movie itself, such as critics score, would not easily allow movie producers to write and design new movies that ended up being popular.",
    "created" : 1471005285696.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "630611348",
    "id" : "1004FF5D",
    "lastKnownWriteTime" : 1470822361,
    "path" : "C:/Users/rob.harrand/Desktop/WORK/Coursera/Bayesian statistics/Project/Bayesian/bayesian_project.Rmd",
    "project_path" : "bayesian_project.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}